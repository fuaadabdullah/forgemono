{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40e93c2b",
   "metadata": {},
   "source": [
    "# Goblin Assistant — RAG Prototype and Prompt Engineering Sandbox\n",
    "\n",
    "This Colab notebook prototypes the full Goblin Assistant workflow:\n",
    "\n",
    "1. Embed documentation\n",
    "2. Vector search (FAISS/Chroma)\n",
    "3. Retrieval-Augmented Generation (RAG)\n",
    "4. LLM answer generation (OpenAI / HF fallback)\n",
    "5. Automated and manual evaluation (ROUGE/BLEU, provenance checks)\n",
    "6. Iteration harness to tune prompts and retrieval\n",
    "7. Prompt engineering playground (system, routing, persona, CoT, tool schemas)\n",
    "\n",
    "Run this notebook in Google Colab for an interactive sandbox. The notebook includes a small built-in sample dataset so you can run end-to-end without external files.\n",
    "\n",
    "---\n",
    "\n",
    "## About This Colab (Demo / R&D Lab)\n",
    "\n",
    "This is a demo environment — not production. Think of it like a pop-up shop for your AI: a place to prototype, experiment, and record demos quickly and safely. Typical uses include:\n",
    "\n",
    "- Show people what Goblin can do with short, repeatable demos\n",
    "- Test and validate the RAG pipeline (embeddings, indexing, retrieval)\n",
    "- Prototype UI ideas before touching the real frontend\n",
    "- Experiment with model swaps, prompt engineering, embeddings, and indexing strategies\n",
    "- Record demos for your site or portfolio and gather examples\n",
    "- Run quick experiments without provisioning long-lived or costly servers\n",
    "\n",
    "Use this notebook as a short-lived sandbox — iterate fast, collect learnings, then port stable patterns back into the backend code paths and the real frontend when they’re ready for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fa1c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Install & Setup (run this cell in Colab)\n",
    "\n",
    "# Use -q to keep output small in notebook\n",
    "!pip install -q sentence-transformers faiss-cpu openai transformers[torch] datasets rouge-score sacrebleu\n",
    "\n",
    "# Optional: chromadb if you prefer\n",
    "!pip install -q chromadb\n",
    "\n",
    "print('Install complete. If you are running in local Python, ensure packages are installed in your environment.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36af6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Import Required Libraries\n",
    "\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import os\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "\n",
    "# Embedding & search\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# FAISS\n",
    "import faiss\n",
    "\n",
    "# LLM calls\n",
    "import openai\n",
    "from transformers import pipeline\n",
    "\n",
    "# Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "import sacrebleu\n",
    "\n",
    "print('Libraries imported')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e4ecaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Sample documentation (small dataset included so notebook runs end-to-end)\n",
    "\n",
    "SAMPLE_DOCS = [\n",
    "    {\n",
    "        'id': 'doc1',\n",
    "        'title': 'Getting started with Goblin Assistant',\n",
    "        'text': \"\"\"\n",
    "Goblin Assistant is an experimental AI assistant built for developer workflows. It accepts user queries, fetches context from documentation, and responds using a retrieval-augmented LLM. Use environment variables to configure backend endpoints.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc2',\n",
    "        'title': 'Installation',\n",
    "        'text': \"\"\"\n",
    "To install Goblin Assistant, clone the repository and run `pnpm install` for frontend and `pip install -r requirements.txt` for backend. After installing, set environment variables like VITE_FASTAPI_URL.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc3',\n",
    "        'title': 'Configuration and API',\n",
    "        'text': \"\"\"\n",
    "The API exposes endpoints at /api/ai and /api/settings. Authentication uses API keys stored in environment variables.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc4',\n",
    "        'title': 'Limitations',\n",
    "        'text': \"\"\"\n",
    "The Hobby plan on Vercel limits serverless functions. For production, use external backend services like Render or consolidate functions.\n",
    "\"\"\",\n",
    "    },\n",
    "    {\n",
    "        'id': 'doc5',\n",
    "        'title': 'Best practices',\n",
    "        'text': \"\"\"\n",
    "When building RAG, chunk long documents, include provenance in responses, and evaluate for hallucinations.\n",
    "\"\"\",\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f'Sample docs loaded: {len(SAMPLE_DOCS)} docs')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90e4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Chunking & Embedding utilities\n",
    "\n",
    "def chunk_text(text: str, chunk_size: int = 200, overlap: int = 50) -> List[str]:\n",
    "    tokens = text.split()\n",
    "    chunks = []\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        chunk = tokens[i:i+chunk_size]\n",
    "        chunks.append(' '.join(chunk))\n",
    "        i += chunk_size - overlap\n",
    "    return chunks\n",
    "\n",
    "# Initialize embedding model (sentence-transformers)\n",
    "print('Loading embedding model...')\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "print('Embedding model loaded')\n",
    "\n",
    "# Prepare corpus: chunk docs and compute embeddings\n",
    "corpus_texts = []\n",
    "corpus_meta = []\n",
    "for d in SAMPLE_DOCS:\n",
    "    chunks = chunk_text(d['text'], chunk_size=60, overlap=10)\n",
    "    for idx, c in enumerate(chunks):\n",
    "        meta = {\n",
    "            'doc_id': d['id'],\n",
    "            'title': d['title'],\n",
    "            'chunk_index': idx,\n",
    "        }\n",
    "        corpus_texts.append(c)\n",
    "        corpus_meta.append(meta)\n",
    "\n",
    "print(f'Corpus prepared with {len(corpus_texts)} chunks')\n",
    "\n",
    "# Compute embeddings\n",
    "corpus_embeddings = embedder.encode(corpus_texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "print('Embeddings computed')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f976f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Build FAISS index and retrieval function\n",
    "\n",
    "# Build faiss index (L2 normalized can be used for cosine similarity)\n",
    "embedding_dim = corpus_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(embedding_dim)\n",
    "index.add(corpus_embeddings)\n",
    "print(f'FAISS index built with {index.ntotal} vectors')\n",
    "\n",
    "def retrieve(query: str, k: int = 3) -> List[Tuple[float, Dict, str]]:\n",
    "    q_emb = embedder.encode([query], convert_to_numpy=True)\n",
    "    D, I = index.search(q_emb, k)\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        meta = corpus_meta[idx]\n",
    "        text = corpus_texts[idx]\n",
    "        results.append((float(score), meta, text))\n",
    "    return results\n",
    "\n",
    "# Quick test\n",
    "print('Retrieving for query: \"How to install Goblin Assistant\"')\n",
    "print(retrieve('How to install Goblin Assistant', k=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97eb056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) RAG prompt templates & prompt engineering examples\n",
    "\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are Goblin Assistant, a concise, developer-focused AI assistant. Use the provided context and cite sources by title when making factual claims. If the answer is not contained in the context, say you don't know and suggest how to find it.\n",
    "\"\"\"\n",
    "\n",
    "ROUTING_PROMPT = \"\"\"\n",
    "If the user query contains keywords like 'deploy', 'build', 'install', route to the deployment instructions tool. Otherwise, use the documentation context.\n",
    "\"\"\"\n",
    "\n",
    "PERSONA_PROMPT = \"\"\"\n",
    "Respond as an experienced developer coach: brief, concrete steps, code snippets when helpful, and safe defaults. Use a friendly but professional tone.\n",
    "\"\"\"\n",
    "\n",
    "COT_INSTRUCTION = \"\"\"\n",
    "When dealing with multi-step problems, think step-by-step and enumerate actions. Start by summarizing what you will do.\n",
    "\"\"\"\n",
    "\n",
    "# Tool schema example for structured invocations (JSON schema)\n",
    "TOOL_SCHEMA = {\n",
    "    'name': 'deploy_tool',\n",
    "    'description': 'Performs deployment steps for project',\n",
    "    'inputs': {\n",
    "        'environment': 'staging|production',\n",
    "        'build_command': 'string',\n",
    "        'env_vars': 'dict'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Example RAG prompt builder\n",
    "def build_rag_prompt(query: str, retrieved: List[Tuple[float, Dict, str]], system=SYSTEM_PROMPT, persona=PERSONA_PROMPT, cot=COT_INSTRUCTION) -> str:\n",
    "    sources = []\n",
    "    for score, meta, text in retrieved:\n",
    "        sources.append(f\"- {meta['title']} (doc={meta['doc_id']}, chunk={meta['chunk_index']})\\n{text[:500]}\")\n",
    "    context_block = '\\n\\n'.join(sources)\n",
    "    prompt = f\"{system}\\n{persona}\\n{cot}\\n\\nCONTEXT:\\n{context_block}\\n\\nUSER QUERY:\\n{query}\\n\\nANSWER:\" \n",
    "    return prompt\n",
    "\n",
    "print('Prompt templates ready')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61be0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) LLM call(s): OpenAI primary, HF fallback\n",
    "\n",
    "# Configure OpenAI key if available\n",
    "OPENAI_KEY = os.environ.get('OPENAI_API_KEY')\n",
    "if OPENAI_KEY:\n",
    "    openai.api_key = OPENAI_KEY\n",
    "\n",
    "\n",
    "def call_openai(prompt: str, model: str = 'gpt-4o-mini', temperature: float = 0.0, max_tokens: int = 512) -> str:\n",
    "    if not OPENAI_KEY:\n",
    "        raise RuntimeError('OPENAI_API_KEY not set')\n",
    "    resp = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=[{'role':'system','content':SYSTEM_PROMPT}, {'role':'user','content':prompt}],\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    return resp['choices'][0]['message']['content']\n",
    "\n",
    "# HF fallback using a small T5/flan model for test/development\n",
    "hf_pipe = None\n",
    "try:\n",
    "    hf_pipe = pipeline('text2text-generation', model='google/flan-t5-small')\n",
    "except Exception as e:\n",
    "    print('HF pipeline not available locally:', e)\n",
    "\n",
    "\n",
    "def call_hf(prompt: str, max_length: int = 256) -> str:\n",
    "    if not hf_pipe:\n",
    "        raise RuntimeError('HF pipeline not available. Install model or set OPENAI_API_KEY')\n",
    "    out = hf_pipe(prompt, max_length=max_length, do_sample=False)\n",
    "    return out[0]['generated_text']\n",
    "\n",
    "print('LLM call functions ready (OpenAI if key present, HF fallback otherwise)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866de7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) RAG driver — retrieve, build prompt, call LLM, return answer with provenance\n",
    "\n",
    "def rag_answer(query: str, k: int = 3, use_openai: bool = True) -> Dict:\n",
    "    retrieved = retrieve(query, k=k)\n",
    "    prompt = build_rag_prompt(query, retrieved)\n",
    "    try:\n",
    "        if use_openai and OPENAI_KEY:\n",
    "            answer = call_openai(prompt)\n",
    "        else:\n",
    "            answer = call_hf(prompt)\n",
    "    except Exception as e:\n",
    "        # simple error recovery: fallback to HF if OpenAI fails\n",
    "        print('LLM call failed:', e)\n",
    "        if hf_pipe:\n",
    "            answer = call_hf(prompt)\n",
    "        else:\n",
    "            answer = 'LLM unavailable. Please set OPENAI_API_KEY or install HF model.'\n",
    "    return {\n",
    "        'query': query,\n",
    "        'answer': answer,\n",
    "        'retrieved': retrieved,\n",
    "    }\n",
    "\n",
    "# Quick RAG test\n",
    "res = rag_answer('How do I install Goblin Assistant?', k=2, use_openai=False)\n",
    "print('Answer:')\n",
    "print(res['answer'])\n",
    "print('\\nSources:')\n",
    "for s in res['retrieved']:\n",
    "    print('-', s[1]['title'], f\"(score={s[0]:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e7319f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Evaluation harness: ROUGE, BLEU, and simple provenance checks\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1','rougeL'], use_stemmer=True)\n",
    "\n",
    "\n",
    "def evaluate_single(reference: str, candidate: str, retrieved: List[Tuple[float, Dict, str]]) -> Dict:\n",
    "    rouge = scorer.score(reference, candidate)\n",
    "    bleu = sacrebleu.sentence_bleu(candidate, [reference]).score\n",
    "    # provenance check: ensure any doc title appears in candidate (very simple heuristic)\n",
    "    provenance_ok = any(meta['title'] in candidate for _, meta, _ in retrieved)\n",
    "    return {\n",
    "        'rouge1': rouge['rouge1'].fmeasure,\n",
    "        'rougeL': rouge['rougeL'].fmeasure,\n",
    "        'bleu': bleu,\n",
    "        'provenance_ok': provenance_ok\n",
    "    }\n",
    "\n",
    "# Example evaluation with a fake reference\n",
    "test_ref = 'To install Goblin Assistant, clone the repo and run pnpm install for frontend and pip install -r requirements.txt for backend.'\n",
    "res = rag_answer('How do I install Goblin Assistant?', k=2, use_openai=False)\n",
    "eval_res = evaluate_single(test_ref, res['answer'], res['retrieved'])\n",
    "print('Evaluation:', eval_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43003521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Iteration harness: sweep prompts and retrieval k\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "prompt_variants = [\n",
    "    {'persona': PERSONA_PROMPT, 'cot': COT_INSTRUCTION},\n",
    "    {'persona': PERSONA_PROMPT + '\\nKeep answers ultra concise (2-3 lines).', 'cot': COT_INSTRUCTION},\n",
    "    {'persona': PERSONA_PROMPT, 'cot': COT_INSTRUCTION + '\\nProvide enumerated steps.'}\n",
    "]\n",
    "\n",
    "ks = [1,2,3]\n",
    "\n",
    "# Example small dataset of queries + refs\n",
    "TEST_SET = [\n",
    "    {'q': 'How to install Goblin Assistant?', 'ref': test_ref},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for variant, k in product(prompt_variants, ks):\n",
    "    # patch templates temporarily\n",
    "    for t in TEST_SET:\n",
    "        retrieved = retrieve(t['q'], k=k)\n",
    "        prompt = build_rag_prompt(t['q'], retrieved, persona=variant['persona'], cot=variant['cot'])\n",
    "        try:\n",
    "            answer = call_hf(prompt)\n",
    "        except Exception:\n",
    "            answer = 'LLM unavailable in notebook environment.'\n",
    "        metrics = evaluate_single(t['ref'], answer, retrieved)\n",
    "        results.append({'persona': variant['persona'][:60], 'k': k, 'metrics': metrics, 'answer': answer})\n",
    "\n",
    "# Show results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame([{'persona': r['persona'], 'k': r['k'], 'rouge1': r['metrics']['rouge1'], 'bleu': r['metrics']['bleu'], 'provenance_ok': r['metrics']['provenance_ok']} for r in results])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c020b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Prompt engineering playground (editable templates)\n",
    "\n",
    "# Editable templates: change these strings and re-run the RAG driver\n",
    "SYSTEM = SYSTEM_PROMPT\n",
    "PERSONA = PERSONA_PROMPT\n",
    "ROUTING = ROUTING_PROMPT\n",
    "COT = COT_INSTRUCTION\n",
    "\n",
    "print('Playground ready. Edit PERSONA/ROUTING/COT and re-run rag_answer() to test.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426741e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Error recovery logic & retry wrapper\n",
    "\n",
    "import functools\n",
    "import random\n",
    "\n",
    "def retry(func=None, *, retries=3, backoff=1.0):\n",
    "    def deco(f):\n",
    "        @functools.wraps(f)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            attempt = 0\n",
    "            while True:\n",
    "                try:\n",
    "                    return f(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    attempt += 1\n",
    "                    if attempt > retries:\n",
    "                        raise\n",
    "                    sleep = backoff * (2 ** (attempt-1)) + random.random()*0.1\n",
    "                    print(f'Retrying after error: {e}. Sleeping {sleep:.2f}s')\n",
    "                    time.sleep(sleep)\n",
    "        return wrapper\n",
    "    return deco\n",
    "\n",
    "# Example usage for calling LLM\n",
    "@retry(retries=3, backoff=1.0)\n",
    "def safe_call_llm(prompt: str):\n",
    "    if OPENAI_KEY:\n",
    "        return call_openai(prompt)\n",
    "    else:\n",
    "        return call_hf(prompt)\n",
    "\n",
    "print('Retry wrapper ready')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900b7915",
   "metadata": {},
   "source": [
    "# How to run in Colab\n",
    "\n",
    "1) Upload this notebook to Colab or open using:\n",
    "\n",
    "https://colab.research.google.com/github/fuaadabdullah/forgemono/blob/main/tools/goblin_assistant_rag_prototype.ipynb\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fuaadabdullah/forgemono/blob/main/tools/goblin_assistant_rag_prototype.ipynb)\n",
    "\n",
    "2) Set secrets: Runtime -> Manage sessions -> set environment variables, or use google.colab to load from Drive\n",
    "\n",
    "> Note: This notebook is a demo environment (R&D sandbox). Do not treat it as production. Use it for quick experiments, prototyping, and demos — then port validated patterns to the backend/frontend for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ac7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Model registry, loader & unified LLM call function for multi-model testing\n",
    "\n",
    "# Registry of available models — extend as needed\n",
    "MODEL_REGISTRY = {\n",
    "    'openai:gpt-4o-mini': {\n",
    "        'type': 'openai',\n",
    "        'model': 'gpt-4o-mini',\n",
    "        'desc': 'OpenAI GPT-4o-mini (requires API key)'\n",
    "    },\n",
    "    'openai:gpt-3.5-turbo': {\n",
    "        'type': 'openai',\n",
    "        'model': 'gpt-3.5-turbo',\n",
    "        'desc': 'OpenAI 3.5 turbo (cheaper fallback)'\n",
    "    },\n",
    "    'hf:flan-t5-small': {\n",
    "        'type': 'hf',\n",
    "        'model': 'google/flan-t5-small',\n",
    "        'desc': 'HF Flan-T5 small (local or HF Inference API)'\n",
    "    },\n",
    "    # Add examples if you have API access or local weights for larger models\n",
    "    # 'hf:llama-2-7b': {...}\n",
    "}\n",
    "\n",
    "# Internal cache to store loaded HF pipelines\n",
    "LOADED_HF_PIPES = {}\n",
    "\n",
    "\n",
    "def load_model_if_needed(model_id: str):\n",
    "    conf = MODEL_REGISTRY.get(model_id)\n",
    "    if not conf:\n",
    "        raise ValueError(f'Model {model_id} not found in registry')\n",
    "    if conf['type'] == 'hf':\n",
    "        if model_id in LOADED_HF_PIPES:\n",
    "            return LOADED_HF_PIPES[model_id]\n",
    "        try:\n",
    "            pipe = pipeline('text2text-generation', model=conf['model'])\n",
    "            LOADED_HF_PIPES[model_id] = pipe\n",
    "            return pipe\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load HF model {conf['model']}:\", e)\n",
    "            # Fallback: try to load from Hugging Face inference API with requests if configured\n",
    "            return None\n",
    "    if conf['type'] == 'openai':\n",
    "        # nothing to load locally; rely on OPENAI_API_KEY being set\n",
    "        return conf['model']\n",
    "    return None\n",
    "\n",
    "\n",
    "def call_model(model_id: str, prompt: str, **kwargs) -> str:\n",
    "    conf = MODEL_REGISTRY.get(model_id)\n",
    "    if not conf:\n",
    "        raise ValueError(f'Model {model_id} not registered')\n",
    "    if conf['type'] == 'openai':\n",
    "        if not OPENAI_KEY:\n",
    "            raise RuntimeError('OPENAI_API_KEY not set for OpenAI model')\n",
    "        model = conf['model']\n",
    "        # Use the common call_openai we already created\n",
    "        return call_openai(prompt=prompt, model=model, temperature=kwargs.get('temperature',0.0), max_tokens=kwargs.get('max_tokens',512))\n",
    "    elif conf['type'] == 'hf':\n",
    "        model_pipe = load_model_if_needed(model_id)\n",
    "        if not model_pipe:\n",
    "            # If local pipeline fails, attempt to call HF Inference API (if HF_TOKEN available)\n",
    "            hf_token = os.environ.get('HF_TOKEN')\n",
    "            if not hf_token:\n",
    "                raise RuntimeError('HF pipeline not available and HF_TOKEN not provided for Inference API fallback')\n",
    "            # HF Inference API call\n",
    "            import requests\n",
    "            hf_api_url = f'https://api-inference.huggingface.co/models/{conf[\"model\"]}'\n",
    "            headers = {'Authorization': f'Bearer {hf_token}'}\n",
    "            payload = {\"inputs\": prompt, \"options\": {\"wait_for_model\": True}, \"parameters\": {\"max_new_tokens\": kwargs.get('max_tokens',256)}}\n",
    "            r = requests.post(hf_api_url, json=payload, headers=headers)\n",
    "            if r.status_code != 200:\n",
    "                raise RuntimeError(f'HF Inference API failed: {r.status_code} {r.text}')\n",
    "            out = r.json()\n",
    "            # HF inference API returns a list of results by default\n",
    "            if isinstance(out, list):\n",
    "                return out[0].get('generated_text', '')\n",
    "            return out.get('generated_text', '')\n",
    "        # Local HF pipeline\n",
    "        out = model_pipe(prompt, max_length=kwargs.get('max_tokens',256), do_sample=False)\n",
    "        if isinstance(out, list):\n",
    "            return out[0].get('generated_text', '')\n",
    "        return out\n",
    "    else:\n",
    "        raise ValueError('Unsupported model type')\n",
    "\n",
    "print('Model registry & loader ready. Registered models:')\n",
    "for mid, cfg in MODEL_REGISTRY.items():\n",
    "    print('-', mid, cfg['desc'])\n",
    "\n",
    "# Quick test using the TF fallback (HF) if OpenAI is not available\n",
    "if 'hf:flan-t5-small' in MODEL_REGISTRY:\n",
    "    try:\n",
    "        _ = load_model_if_needed('hf:flan-t5-small')\n",
    "        print('Local HF model flan-t5-small ready for calls (or API fallback configured)')\n",
    "    except Exception as e:\n",
    "        print('HF model not available locally:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638469a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Multi-model RAG comparison helper\n",
    "\n",
    "from typing import Any\n",
    "\n",
    "\n",
    "def compare_models_on_query(query: str, model_ids: List[str], k: int = 3) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Run RAG for each model in model_ids and return results with evaluations\"\"\"\n",
    "    outputs = []\n",
    "    for model_id in model_ids:\n",
    "        print(f'Running model: {model_id} — retrieval k={k}')\n",
    "        retrieved = retrieve(query, k=k)\n",
    "        prompt = build_rag_prompt(query, retrieved, persona=PERSONA, cot=COT)\n",
    "        try:\n",
    "            answer = call_model(model_id, prompt)\n",
    "        except Exception as e:\n",
    "            answer = f'ERROR: {e}'\n",
    "        metrics = evaluate_single(test_ref, answer, retrieved)\n",
    "        outputs.append({'model_id': model_id, 'answer': answer, 'metrics': metrics, 'retrieved': retrieved})\n",
    "    return outputs\n",
    "\n",
    "# Example: compare three models on the test query\n",
    "compare_result = compare_models_on_query('How do I install Goblin Assistant?', ['hf:flan-t5-small', 'openai:gpt-3.5-turbo'], k=2)\n",
    "for r in compare_result:\n",
    "    print('\\n---')\n",
    "    print('Model:', r['model_id'])\n",
    "    print('Answer:', r['answer'])\n",
    "    print('Metrics:', r['metrics'])\n",
    "    print('Sources:', [s[1]['title'] for s in r['retrieved']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf903fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Simple IPyWidgets UI (optional: run in Colab/local to interactively select models)\n",
    "\n",
    "try:\n",
    "    from ipywidgets import SelectMultiple, Button, HBox, VBox, Text, Output, IntSlider\n",
    "    from IPython.display import display\n",
    "\n",
    "    available_models = list(MODEL_REGISTRY.keys())\n",
    "    model_selector = SelectMultiple(options=available_models, value=[available_models[0]], description='Models')\n",
    "    query_input = Text(value='How do I install Goblin Assistant?', description='Query')\n",
    "    k_slider = IntSlider(value=2, min=1, max=5, description='k')\n",
    "    run_btn = Button(description='Run Comparison', button_style='primary')\n",
    "    out = Output()\n",
    "\n",
    "    def on_run(b):\n",
    "        with out:\n",
    "            out.clear_output()\n",
    "            selected = list(model_selector.value)\n",
    "            q = query_input.value\n",
    "            k = k_slider.value\n",
    "            res = compare_models_on_query(q, selected, k=k)\n",
    "            for r in res:\n",
    "                print('---')\n",
    "                print('Model:', r['model_id'])\n",
    "                print('Answer:')\n",
    "                print(r['answer'])\n",
    "                print('Metrics:', r['metrics'])\n",
    "                print('Sources:', [s[1]['title'] for s in r['retrieved']])\n",
    "\n",
    "    run_btn.on_click(on_run)\n",
    "\n",
    "    ui = VBox([query_input, HBox([model_selector, k_slider]), run_btn, out])\n",
    "    display(ui)\n",
    "\n",
    "except Exception as e:\n",
    "    print('IPyWidgets not available or failed to render:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63496ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) End-to-end prototype workflow\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Utility: normalize vectors for inner product similarity\n",
    "def normalize_vectors(vs: np.ndarray) -> np.ndarray:\n",
    "    norms = np.linalg.norm(vs, axis=1, keepdims=True) + 1e-10\n",
    "    return vs / norms\n",
    "\n",
    "\n",
    "def build_faiss_index(docs: List[Dict[str, Any]], embed_model: SentenceTransformer):\n",
    "    \"\"\"Embed docs and build a FAISS index (Inner Product on normalized vectors).\n",
    "\n",
    "    Returns: index, embeddings, id_map\n",
    "    \"\"\"\n",
    "    texts = [d[\"text\"] for d in docs]\n",
    "    embeddings = embed_model.encode(texts, convert_to_numpy=True)\n",
    "    embeddings = normalize_vectors(embeddings)\n",
    "\n",
    "    dim = embeddings.shape[1]\n",
    "    index = faiss.IndexFlatIP(dim)  # inner product on normalized vectors approximates cosine\n",
    "    index.add(embeddings)\n",
    "\n",
    "    id_map = [d[\"id\"] for d in docs]\n",
    "    return index, embeddings, id_map\n",
    "\n",
    "\n",
    "def search_index(query: str, index, embed_model: SentenceTransformer, id_map: List[str], docs: List[Dict[str, Any]], top_k: int = 3):\n",
    "    qv = embed_model.encode([query], convert_to_numpy=True)\n",
    "    qv = normalize_vectors(qv)\n",
    "    D, I = index.search(qv, top_k)\n",
    "    results = []\n",
    "    for s, idx in zip(D[0], I[0]):\n",
    "        results.append({\n",
    "            \"score\": float(s),\n",
    "            \"id\": id_map[int(idx)],\n",
    "            \"text\": docs[int(idx)][\"text\"],\n",
    "            \"title\": docs[int(idx)].get(\"title\", \"\")\n",
    "        })\n",
    "    return results\n",
    "\n",
    "\n",
    "def build_rag_prompt(query: str, retrieved: List[Dict[str, Any]], template: str = None) -> str:\n",
    "    \"\"\"Join the retrieved text into a RAG-aware prompt.\n",
    "\n",
    "    The template can be used to configure system instructions.\n",
    "    \"\"\"\n",
    "    ctx = \"\\n\\n---\\n\".join([f\"[{r['id']}] {r['title']}\\n{r['text']}\" for r in retrieved])\n",
    "    if template is None:\n",
    "        system = (\n",
    "            \"You are Goblin Assistant. Use only the given context to answer the user's question. \"\n",
    "            \"Be concise and cite the source IDs you used.\\n\\n\"\n",
    "        )\n",
    "    else:\n",
    "        system = template + \"\\n\\n\"\n",
    "\n",
    "    prompt = f\"{system}Context:\\n{ctx}\\n\\nUser Query: {query}\\n\\nAnswer:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# LLM call: OpenAI (if key present) otherwise local HF pipeline (text-generation) as fallback\n",
    "\n",
    "def call_llm(prompt: str, temperature: float = 0.0, max_tokens: int = 256):\n",
    "    \"\"\"Return JSON: {'text': str, 'provider': 'openai'|'hf', 'raw': ...}\n",
    "\n",
    "    If OPENAI_API_KEY is present, prefer OpenAI Chat; else use HF text-generation.\n",
    "    \"\"\"\n",
    "    openai_key = os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"OPENAI_API_KEY_LOCAL\")\n",
    "    if openai_key:\n",
    "        try:\n",
    "            openai.api_key = openai_key\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            text = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            return {\"text\": text, \"provider\": \"openai\", \"raw\": response}\n",
    "        except Exception as e:\n",
    "            print(\"OpenAI request failed, falling back to HF: \", e)\n",
    "\n",
    "    # Fallback to HF pipeline\n",
    "    try:\n",
    "        pipeline_inst = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
    "        res = pipeline_inst(prompt, max_length=min(1024, len(prompt.split()) + 200))\n",
    "        text = res[0][\"generated_text\"][len(prompt) :].strip()\n",
    "        return {\"text\": text, \"provider\": \"hf\", \"raw\": res}\n",
    "    except Exception as e:\n",
    "        return {\"text\": \"(LLM call failed: \" + str(e) + \")\", \"provider\": \"none\", \"raw\": str(e)}\n",
    "\n",
    "\n",
    "# Evaluation helpers\n",
    "scorer = rouge_scorer.RougeScorer([\"rouge1\", \"rouge2\", \"rougeL\"], use_stemmer=True)\n",
    "\n",
    "\n",
    "def eval_texts(reference: str, candidate: str):\n",
    "    \"\"\"Return evaluation metrics: rouge and BLEU (sacrebleu)\"\"\"\n",
    "    rouge_scores = scorer.score(reference, candidate)\n",
    "    bleu = sacrebleu.sentence_bleu(candidate, [reference]).score\n",
    "    return {\n",
    "        \"rouge1_f\": rouge_scores[\"rouge1\"].fmeasure,\n",
    "        \"rouge2_f\": rouge_scores[\"rouge2\"].fmeasure,\n",
    "        \"rougeL_f\": rouge_scores[\"rougeL\"].fmeasure,\n",
    "        \"bleu\": float(bleu),\n",
    "    }\n",
    "\n",
    "\n",
    "def provenance_check(candidate: str, retrieved: List[Dict[str, Any]]):\n",
    "    \"\"\"Return a list of source ids that best match substrings in the candidate.\n",
    "\n",
    "    Simple method: check for substring occurrences of 8+ char sequences from retrieved doc.\n",
    "    \"\"\"\n",
    "    provenance = []\n",
    "    for r in retrieved:\n",
    "        # pick a unique snippet to match\n",
    "        snippet = r[\"text\"][:120].strip()\n",
    "        if snippet and snippet in candidate:\n",
    "            provenance.append(r[\"id\"])\n",
    "    return provenance\n",
    "\n",
    "\n",
    "# High-level orchestrator\n",
    "\n",
    "def prototype_goblin_workflow(query: str, docs: List[Dict[str, Any]], embed_model_name: str = \"all-MiniLM-L6-v2\", top_k: int = 3, template: str = None, temperature: float = 0.0):\n",
    "    embed_model = SentenceTransformer(embed_model_name)\n",
    "    idx, embeddings, id_map = build_faiss_index(docs, embed_model)\n",
    "    retrieved = search_index(query, idx, embed_model, id_map, docs, top_k=top_k)\n",
    "\n",
    "    prompt = build_rag_prompt(query, retrieved, template)\n",
    "    llm_result = call_llm(prompt, temperature=temperature)\n",
    "\n",
    "    # for evaluation, we expect the notebook user to provide a reference answer; for demo we pull sample mapping\n",
    "    reference_map = {\n",
    "        \"What is Goblin Assistant?\": \"Goblin Assistant is an experimental AI assistant built for developer workflows. It fetches conversation context and answers by combining docs and LLMs.\",\n",
    "        \"How do I install Goblin Assistant?\": \"Clone the repo; for frontend: pnpm install; for backend: pip install -r requirements.txt; set env vars like VITE_FASTAPI_URL and run uvicorn.\",\n",
    "    }\n",
    "\n",
    "    reference = reference_map.get(query, \"\")\n",
    "    if reference:\n",
    "        eval_metrics = eval_texts(reference, llm_result[\"text\"])  # computed on the simple sample mapping\n",
    "    else:\n",
    "        eval_metrics = {}\n",
    "\n",
    "    prov = provenance_check(llm_result[\"text\"], retrieved)\n",
    "\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"retrieved\": retrieved,\n",
    "        \"prompt\": prompt,\n",
    "        \"llm_result\": llm_result,\n",
    "        \"metrics\": eval_metrics,\n",
    "        \"provenance\": prov,\n",
    "    }\n",
    "\n",
    "\n",
    "# Example usage\n",
    "EXAMPLE_QUERIES = [\n",
    "    \"What is Goblin Assistant?\",\n",
    "    \"How do I install Goblin Assistant?\",\n",
    "]\n",
    "\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "index, _, idmap = build_faiss_index(SAMPLE_DOCS, embedder)\n",
    "\n",
    "for q in EXAMPLE_QUERIES:\n",
    "    print(\"\\n---\\nRunning query: \", q)\n",
    "    result = prototype_goblin_workflow(q, SAMPLE_DOCS, embed_model_name=\"all-MiniLM-L6-v2\", top_k=3)\n",
    "    print(\"Answer (provider=\", result[\"llm_result\"][\"provider\"], \"):\\n\", result[\"llm_result\"][\"text\"])\n",
    "    print(\"Retrieved doc IDs:\", [r[\"id\"] for r in result[\"retrieved\"]])\n",
    "    if result[\"metrics\"]:\n",
    "        print(\"Metrics:\", result[\"metrics\"])\n",
    "    print(\"Provenance (sources quoted):\", result[\"provenance\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336edef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Iteration harness: tune retrieval and prompts\n",
    "from itertools import product\n",
    "\n",
    "PROMPT_TEMPLATES = [\n",
    "    None,\n",
    "    \"You are Goblin Assistant. Use only the given context sections to answer, cite sources by ID.\",\n",
    "    \"You are a helpful assistant. Provide short, step-by-step instructions if asked and show which docs you used by id.\",\n",
    "]\n",
    "\n",
    "\n",
    "def grid_search_workflow(queries: List[str], docs: List[Dict[str, Any]], embed_model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "    results = []\n",
    "    top_ks = [1, 3, 5]\n",
    "    temps = [0.0, 0.3]\n",
    "    for q in queries:\n",
    "        for top_k, temp, template in product(top_ks, temps, PROMPT_TEMPLATES):\n",
    "            out = prototype_goblin_workflow(q, docs, embed_model_name=embed_model_name, top_k=top_k, template=template, temperature=temp)\n",
    "            # naive cooked detection: fraction of retrieved docs used as provenance\n",
    "            prov_frac = float(len(out[\"provenance\"]) / max(1, len(out[\"retrieved\"])))\n",
    "            out[\"provenance_fraction\"] = prov_frac\n",
    "            out[\"cooked_flag\"] = prov_frac < 0.33 and out[\"llm_result\"][\"provider\"] == \"openai\"\n",
    "            results.append({\n",
    "                \"query\": q,\n",
    "                \"top_k\": top_k,\n",
    "                \"temp\": temp,\n",
    "                \"template\": template,\n",
    "                \"metrics\": out[\"metrics\"],\n",
    "                \"prov_frac\": prov_frac,\n",
    "                \"cooked\": out[\"cooked_flag\"],\n",
    "                \"llm_provider\": out[\"llm_result\"][\"provider\"],\n",
    "            })\n",
    "    return results\n",
    "\n",
    "\n",
    "# Run grid search\n",
    "print('Running grid search for sample queries... this may call remote LLMs if API keys are present')\n",
    "search_results = grid_search_workflow(EXAMPLE_QUERIES, SAMPLE_DOCS)\n",
    "\n",
    "# Sort by Rouge-L fmeasure and present top results, filter out empty metrics\n",
    "valid_results = [r for r in search_results if r[\"metrics\"]]\n",
    "sorted_results = sorted(valid_results, key=lambda r: r[\"metrics\"][\"rougeL_f\"], reverse=True)\n",
    "\n",
    "print('\\nTop parameter combos (by Rouge-L):')\n",
    "for r in sorted_results[:10]:\n",
    "    print(f\"Query: {r['query']}, top_k={r['top_k']}, temp={r['temp']}, template={'default' if r['template'] is None else 'custom'}, provider={r['llm_provider']}, rougeL={r['metrics']['rougeL_f']:.3f}, prov_frac={r['prov_frac']:.2f}, cooked={r['cooked']}\")\n",
    "\n",
    "# Show all combos that appear to be 'cooked' (low provenance)\n",
    "cooked = [r for r in search_results if r['cooked']]\n",
    "print('\\nPotentially cooked results (prov_frac < 0.33):')\n",
    "for r in cooked:\n",
    "    print(f\"Query: {r['query']}, top_k={r['top_k']}, provider={r['llm_provider']}, prov_frac={r['prov_frac']:.3f}, rougeL={r.get('metrics', {}).get('rougeL_f', None)}\")\n",
    "\n",
    "print('\\nGrid search complete — examine results and adjust templates/top_k/temperature.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14ba4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19) Auto-iterate until grounded (naive loop)\n",
    "\n",
    "def iterate_until_grounded(query: str, docs: List[Dict[str, Any]], max_top_k: int = 10, templates: List[str] = PROMPT_TEMPLATES, temps: List[float] = [0.0, 0.3], embed_model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "    # We'll iterate over top_k first and then try templates/temps for the smallest top_k that gives decent provenance\n",
    "    for top_k in range(1, max_top_k + 1):\n",
    "        for template in templates:\n",
    "            for t in temps:\n",
    "                out = prototype_goblin_workflow(query, docs, embed_model_name=embed_model_name, top_k=top_k, template=template, temperature=t)\n",
    "                prov_frac = float(len(out[\"provenance\"]) / max(1, len(out[\"retrieved\"])))\n",
    "                print(f\"top_k={top_k}, temp={t}, template={'default' if template is None else 'custom'} -> prov_frac={prov_frac:.2f}, provider={out['llm_result']['provider']}\")\n",
    "                if prov_frac >= 0.66:  # heuristics: at least 66% of retrieved docs are referenced\n",
    "                    print(\"Grounded answer found — returning result\")\n",
    "                    return out\n",
    "    print(\"No sufficiently grounded answer found in grid. Consider adding more specific docs or changing templates.\")\n",
    "    return None\n",
    "\n",
    "# Example: iterate for a query until grounded\n",
    "print(iterate_until_grounded(\"What is Goblin Assistant?\", SAMPLE_DOCS, max_top_k=5))\n",
    "\n",
    "# If the iterative function returns None, try improving docs, chunking, or templates.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
