apiVersion: v1
kind: Namespace
metadata:
  name: goblinos-ai
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ollama-inference
  namespace: goblinos-ai
  labels:
    app: ollama-inference
spec:
  replicas: 3
  selector:
    matchLabels:
      app: ollama-inference
  template:
    metadata:
      labels:
        app: ollama-inference
    spec:
      containers:
      - name: ollama
        image: ollama/local-inference:latest
        env:
        - name: AZURE_AI_ENDPOINT
          value: "https://your-azure-ai-endpoint.openai.azure.com/"
        - name: GCP_AI_ENDPOINT
          value: "https://your-gcp-ai-endpoint.googleapis.com/"
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 11434
          name: ollama-api
        resources:
          limits:
            cpu: "1000m"
            memory: "1Gi"
          requests:
            cpu: "500m"
            memory: "512Mi"
        # Add readiness and liveness probes
        readinessProbe:
          httpGet:
            path: /api/tags
            port: ollama-api
          initialDelaySeconds: 30
          periodSeconds: 10
        livenessProbe:
          httpGet:
            path: /api/tags
            port: ollama-api
          initialDelaySeconds: 60
          periodSeconds: 30
      - name: metrics-sidecar
        image: node:18-alpine
        ports:
        - containerPort: 9090
          name: metrics
        env:
        - name: NODE_ENV
          value: "production"
        command: ["node", "/app/metrics-server.js"]
        volumeMounts:
        - name: metrics-script
          mountPath: /app
        resources:
          limits:
            cpu: "100m"
            memory: "128Mi"
          requests:
            cpu: "50m"
            memory: "64Mi"
      volumes:
      - name: metrics-script
        configMap:
          name: ollama-metrics-script
---
apiVersion: v1
kind: Service
metadata:
  name: ollama-service
  namespace: goblinos-ai
spec:
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 8080
  - name: ollama-api
    port: 11434
    protocol: TCP
    targetPort: 11434
  - name: metrics
    port: 9090
    protocol: TCP
    targetPort: 9090
  selector:
    app: ollama-inference
  type: ClusterIP
---
# Horizontal Pod Autoscaler for CPU and memory scaling
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: ollama-hpa
  namespace: goblinos-ai
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: ollama-inference
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
  - type: Pods
    pods:
      metric:
        name: litebrain_complexity_score
      target:
        type: AverageValue
        averageValue: "50"
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 50
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 2
        periodSeconds: 60
---
# Prometheus metrics service monitor
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: ollama-servicemonitor
  namespace: goblinos-ai
  labels:
    app: ollama-inference
spec:
  selector:
    matchLabels:
      app: ollama-inference
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
---
# ConfigMap for metrics script
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-metrics-script
  namespace: goblinos-ai
data:
  metrics-server.js: |
    const express = require('express');
    const { register, collectDefaultMetrics, Gauge } = require('prom-client');

    // Enable default metrics
    collectDefaultMetrics();

    // Custom complexity metric
    const complexityScore = new Gauge({
      name: 'litebrain_complexity_score',
      help: 'Current complexity score (0-100)',
      labelNames: ['model']
    });

    // Simulate complexity based on time (for demo)
    setInterval(() => {
      const score = Math.floor(Math.random() * 100);
      complexityScore.set({ model: 'ollama' }, score);
    }, 30000);

    const app = express();

    app.get('/metrics', async (req, res) => {
      res.set('Content-Type', register.contentType);
      res.end(await register.metrics());
    });

    app.get('/health', (req, res) => {
      res.json({ status: 'healthy' });
    });

    app.listen(9090, () => {
      console.log('Metrics server running on port 9090');
    });
---
# ConfigMap for scaling policies
apiVersion: v1
kind: ConfigMap
metadata:
  name: ollama-scaling-config
  namespace: goblinos-ai
data:
  scaling-policy.json: |
    {
      "complexityThresholds": {
        "low": 30,
        "medium": 50,
        "high": 80
      },
      "modelResourceWeights": {
        "qwen2.5:3b": 1.0,
        "deepseek-r1": 2.5,
        "codellama": 2.0,
        "llama2:7b": 1.8
      },
      "scaleUpFactors": {
        "low": 1.2,
        "medium": 1.5,
        "high": 2.0
      }
    }
