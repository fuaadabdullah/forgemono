{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e51fa227",
   "metadata": {},
   "source": [
    "# Ollama on Google Colab with Google Drive + Public Access\n",
    "\n",
    "Run Ollama AI models in Google Colab with persistent storage via Google Drive and public web access via ngrok.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. **Mount Google Drive** (for persistent model storage)\n",
    "2. **Install Ollama** in the Colab VM\n",
    "3. **Configure model directory** to use Drive\n",
    "4. **Download models** (stored in Drive for reuse)\n",
    "5. **Start Ollama server**\n",
    "6. **Expose publicly** with ngrok tunnel\n",
    "\n",
    "## ‚ö†Ô∏è Limitations\n",
    "\n",
    "- **Session timeout**: ~90 min idle, ~12 hours max (free Colab)\n",
    "- **Resource constraints**: Limited RAM/CPU - big models may not fit\n",
    "- **Not production-safe**: Unpredictable uptime and latency\n",
    "- **Treat as demo/sandbox**: Great for testing, not production use\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "- Google account with Colab access\n",
    "- Google Drive space for models (GGUF files can be large)\n",
    "- ngrok account (free tier available)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc9ddf4",
   "metadata": {},
   "source": [
    "## 1. Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to store Ollama models persistently. This prevents re-downloading large model files every session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339bf682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# Create Ollama directory in Drive (if it doesn't exist)\n",
    "import os\n",
    "\n",
    "ollama_drive_path = \"/content/drive/MyDrive/ollama\"\n",
    "os.makedirs(ollama_drive_path, exist_ok=True)\n",
    "os.makedirs(f\"{ollama_drive_path}/models\", exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Google Drive mounted and Ollama directory created at: {ollama_drive_path}\")\n",
    "print(f\"üìÅ Models will be stored in: {ollama_drive_path}/models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add098ba",
   "metadata": {},
   "source": [
    "## 2. Install Ollama\n",
    "\n",
    "Install Ollama in the Colab environment. We'll use the official installation script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dd485b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Ollama\n",
    "!curl -fsSL https://ollama.ai/install.sh | sh\n",
    "\n",
    "# Verify installation\n",
    "!ollama --version\n",
    "\n",
    "print(\"‚úÖ Ollama installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a9be020",
   "metadata": {},
   "source": [
    "## 3. Configure Ollama to Use Google Drive\n",
    "\n",
    "Set up Ollama to use your Google Drive directory for model storage instead of the default Colab location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3bc554",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables to use Drive for Ollama data\n",
    "import os\n",
    "\n",
    "ollama_drive_path = \"/content/drive/MyDrive/ollama\"\n",
    "\n",
    "# Set OLLAMA_MODELS to point to Drive\n",
    "os.environ[\"OLLAMA_MODELS\"] = f\"{ollama_drive_path}/models\"\n",
    "\n",
    "# Create symlink from default location to Drive (for compatibility)\n",
    "default_models_path = \"/root/.ollama/models\"\n",
    "os.makedirs(\"/root/.ollama\", exist_ok=True)\n",
    "\n",
    "# Remove existing symlink/directory if it exists\n",
    "if os.path.exists(default_models_path):\n",
    "    if os.path.islink(default_models_path):\n",
    "        os.unlink(default_models_path)\n",
    "    else:\n",
    "        import shutil\n",
    "\n",
    "        shutil.rmtree(default_models_path)\n",
    "\n",
    "# Create symlink to Drive\n",
    "os.symlink(f\"{ollama_drive_path}/models\", default_models_path)\n",
    "\n",
    "print(f\"‚úÖ Ollama configured to use Google Drive: {ollama_drive_path}/models\")\n",
    "print(f\"üìÅ Symlink created: {default_models_path} -> {ollama_drive_path}/models\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9f5148",
   "metadata": {},
   "source": [
    "## 4. Download Models\n",
    "\n",
    "Download your desired models. Since they're stored in Google Drive, they'll persist across sessions.\n",
    "\n",
    "**Popular lightweight models for Colab:**\n",
    "- `qwen2.5:3b` - Fast, capable 3B parameter model\n",
    "- `deepseek-coder:1.3b` - Code-focused model\n",
    "- `gemma:2b` - Google's lightweight model\n",
    "- `phi3:3.8b` - Microsoft's efficient model\n",
    "\n",
    "**Note:** Large models (>7B) may not fit in Colab's RAM/CPU constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eae69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your models (uncomment the ones you want)\n",
    "models_to_download = [\n",
    "    \"qwen2.5:3b\",  # Fast, capable general model\n",
    "    \"deepseek-coder:1.3b\",  # Code-focused model\n",
    "    \"gemma:2b\",  # Google's lightweight model\n",
    "    # \"phi3:3.8b\",       # Microsoft's efficient model (uncomment if needed)\n",
    "    # \"llama3:8b\",       # Large model - may not fit in Colab (uncomment if you have Pro)\n",
    "]\n",
    "\n",
    "print(\"üöÄ Starting model downloads...\")\n",
    "print(f\"üìÅ Models will be saved to Google Drive: {ollama_drive_path}/models\")\n",
    "print()\n",
    "\n",
    "for model in models_to_download:\n",
    "    print(f\"‚¨áÔ∏è Downloading {model}...\")\n",
    "    !ollama pull {model}\n",
    "    print(f\"‚úÖ {model} downloaded successfully!\")\n",
    "    print()\n",
    "\n",
    "print(\"üéâ All models downloaded!\")\n",
    "print(\"üìã Available models:\")\n",
    "!ollama list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145759cc",
   "metadata": {},
   "source": [
    "## 5. Start Ollama Server\n",
    "\n",
    "Start the Ollama server in the background. It will run on localhost:11434."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184924c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Ollama server in background\n",
    "import subprocess\n",
    "import time\n",
    "\n",
    "print(\"üöÄ Starting Ollama server...\")\n",
    "\n",
    "# Start server in background\n",
    "process = subprocess.Popen(\n",
    "    [\"ollama\", \"serve\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    ")\n",
    "\n",
    "# Wait a moment for server to start\n",
    "time.sleep(3)\n",
    "\n",
    "# Check if server is running\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"curl\", \"-s\", \"http://localhost:11434/api/tags\"],\n",
    "        capture_output=True,\n",
    "        text=True,\n",
    "        timeout=5,\n",
    "    )\n",
    "    if result.returncode == 0:\n",
    "        print(\"‚úÖ Ollama server started successfully!\")\n",
    "        print(\"üåê Server running on: http://localhost:11434\")\n",
    "    else:\n",
    "        print(\"‚ùå Server may not be responding yet, but process started\")\n",
    "except:\n",
    "    print(\n",
    "        \"‚ö†Ô∏è Server process started, but health check failed (this is normal initially)\"\n",
    "    )\n",
    "\n",
    "# Test with a simple model\n",
    "print(\"\\nüß™ Testing with a simple model...\")\n",
    "!ollama run qwen2.5:3b \"Hello!\" --format json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323ce5ae",
   "metadata": {},
   "source": [
    "## 6. Set Up Public Access with ngrok\n",
    "\n",
    "Expose your Ollama server to the internet using ngrok. This gives you a public URL that forwards to your Colab instance.\n",
    "\n",
    "**Requirements:**\n",
    "- Free ngrok account (sign up at ngrok.com)\n",
    "- ngrok auth token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e5530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install ngrok\n",
    "!pip install pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import os\n",
    "\n",
    "# Set your ngrok auth token (replace with your actual token)\n",
    "# Get your token from: https://dashboard.ngrok.com/get-started/your-authtoken\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_AUTH_TOKEN_HERE\"  # ‚ö†Ô∏è Replace with your token!\n",
    "\n",
    "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_AUTH_TOKEN_HERE\":\n",
    "    print(\"‚ùå Please set your ngrok auth token!\")\n",
    "    print(\"1. Sign up at https://ngrok.com\")\n",
    "    print(\"2. Get your auth token from https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "    print(\"3. Replace YOUR_NGROK_AUTH_TOKEN_HERE with your actual token\")\n",
    "else:\n",
    "    # Authenticate ngrok\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    \n",
    "    # Start tunnel to Ollama port\n",
    "    print(\"üåê Starting ngrok tunnel to Ollama server...\")\n",
    "    tunnel = ngrok.connect(11434, \"http\")\n",
    "    \n",
    "    public_url = tunnel.public_url\n",
    "    print(f\"‚úÖ Public URL: {public_url}\")\n",
    "    print(f\"üåç Your Ollama API is now accessible at: {public_url}/v1/chat/completions\")\n",
    "    print()\n",
    "    print(\"üìã Example API call:\")\n",
    "    print(f\"curl -X POST {public_url}/v1/chat/completions \\\")\n",
    "    print(\"  -H 'Content-Type: application/json' \\\")\n",
    "    print(\"  -d '{\\\"model\\\": \\\"qwen2.5:3b\\\", \\\"messages\\\": [{\\\"role\\\": \\\"user\\\", \\\"content\\\": \\\"Hello!\\\"}]}'\")\n",
    "    \n",
    "    # Keep tunnel alive\n",
    "    print(\"\\nüîÑ Tunnel is active. Keep this cell running to maintain the connection.\")\n",
    "    print(\"‚ö†Ô∏è Colab sessions timeout, so this URL will only work while the session is active.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b190a9e",
   "metadata": {},
   "source": [
    "## 7. Alternative: Pinggy Tunnel (if ngrok doesn't work)\n",
    "\n",
    "If ngrok has issues, you can use Pinggy as an alternative tunneling service."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9afc2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Pinggy tunnel\n",
    "# Download and install Pinggy\n",
    "!wget https://pinggy.io/cli/pinggy_0.1.1_linux_amd64.tar.gz\n",
    "!tar -xzf pinggy_0.1.1_linux_amd64.tar.gz\n",
    "!chmod +x pinggy\n",
    "\n",
    "print(\"üåê Starting Pinggy tunnel...\")\n",
    "print(\"üìã Your public URL will be displayed below:\")\n",
    "print(\"‚ö†Ô∏è Keep this cell running to maintain the tunnel\")\n",
    "print()\n",
    "\n",
    "# Start Pinggy tunnel (this will run indefinitely)\n",
    "!./pinggy -p 11434 http\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720b468",
   "metadata": {},
   "source": [
    "## 8. Test Your Public API\n",
    "\n",
    "Test that your public Ollama API is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12873ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the public API\n",
    "import requests\n",
    "import json\n",
    "\n",
    "# Get the public URL (replace with your actual ngrok URL)\n",
    "# If using ngrok, copy the URL from the previous cell\n",
    "# If using Pinggy, copy the URL from the Pinggy output\n",
    "\n",
    "public_url = \"YOUR_PUBLIC_URL_HERE\"  # Replace with your actual URL\n",
    "\n",
    "if public_url == \"YOUR_PUBLIC_URL_HERE\":\n",
    "    print(\"‚ùå Please set your public URL!\")\n",
    "    print(\"Copy the URL from the ngrok or Pinggy output above.\")\n",
    "else:\n",
    "    try:\n",
    "        # Test models endpoint\n",
    "        response = requests.get(f\"{public_url}/api/tags\", timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            models = response.json()\n",
    "            print(\"‚úÖ Public API is working!\")\n",
    "            print(\"üìã Available models:\")\n",
    "            for model in models.get(\"models\", []):\n",
    "                print(f\"  ‚Ä¢ {model['name']}\")\n",
    "        else:\n",
    "            print(f\"‚ùå API returned status code: {response.status_code}\")\n",
    "\n",
    "        # Test chat completion\n",
    "        print(\"\\nüß™ Testing chat completion...\")\n",
    "        payload = {\n",
    "            \"model\": \"qwen2.5:3b\",\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": \"Say hello in 10 words or less.\"}],\n",
    "            \"stream\": False,\n",
    "        }\n",
    "        response = requests.post(\n",
    "            f\"{public_url}/v1/chat/completions\", json=payload, timeout=30\n",
    "        )\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            content = result[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(f\"ü§ñ Response: {content}\")\n",
    "            print(\"‚úÖ Chat completion working!\")\n",
    "        else:\n",
    "            print(f\"‚ùå Chat completion failed: {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Test failed: {e}\")\n",
    "        print(\"üí° Make sure:\")\n",
    "        print(\"   1. Ollama server is running\")\n",
    "        print(\"   2. The tunnel is active\")\n",
    "        print(\"   3. You copied the correct public URL\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad854cb2",
   "metadata": {},
   "source": [
    "## 9. Integration with Goblin Assistant\n",
    "\n",
    "Configure your Goblin Assistant backend to use this Colab-hosted Ollama instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc6920f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Goblin Assistant\n",
    "public_url = \"YOUR_PUBLIC_URL_HERE\"  # Replace with your actual URL\n",
    "\n",
    "if public_url != \"YOUR_PUBLIC_URL_HERE\":\n",
    "    print(\"üîß Goblin Assistant Configuration:\")\n",
    "    print(\"=\" * 50)\n",
    "    print()\n",
    "    print(\"# Automatic setup using the integration script:\")\n",
    "    print(f\"cd /path/to/ForgeMonorepo\")\n",
    "    print(f\"python3 setup_colab_ollama_integration.py --colab-url {public_url} --auto-test\")\n",
    "    print()\n",
    "    print(\"# Or manually update providers.toml with:\")\n",
    "    print(\"[providers.ollama_colab]\")\n",
    "    print('name = \"Ollama (Colab)\"')\n",
    "    print(f'endpoint = \"{public_url}\"')\n",
    "    print('capabilities = [\"chat\", \"reasoning\", \"code\", \"embedding\"]')\n",
    "    print('models = [\"qwen2.5:3b\", \"deepseek-coder:1.3b\", \"gemma:2b\"]')\n",
    "    print(\"priority_tier = 0\")\n",
    "    print(\"cost_score = 0.0\")\n",
    "    print(\"default_timeout_ms = 30000\")\n",
    "    print(\"bandwidth_score = 0.5\")\n",
    "    print(\"supports_cot = false\")\n",
    "    print()\n",
    "    print(\"‚ö†Ô∏è Remember: Colab sessions timeout, so this endpoint is temporary!\")\n",
    "    print(\"üîÑ Use the setup script to update the URL when you restart Colab.\")\n",
    "else:\n",
    "    print(\"‚ùå Please set your public URL first!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b4b88e",
   "metadata": {},
   "source": [
    "## üìö Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Colab session crashed/restarted**\n",
    "   - Models in Drive persist, but you need to remount Drive and restart Ollama\n",
    "   - Run cells 1-5 again to get back up and running\n",
    "\n",
    "2. **Out of RAM/CPU**\n",
    "   - Use smaller models (3B or less parameters)\n",
    "   - Upgrade to Colab Pro for more resources\n",
    "   - Close other Colab tabs\n",
    "\n",
    "3. **ngrok/Pinggy connection issues**\n",
    "   - Check your auth token is correct\n",
    "   - Try the alternative tunneling service\n",
    "   - Make sure Ollama is running on port 11434\n",
    "\n",
    "4. **Models not persisting**\n",
    "   - Ensure Drive is mounted correctly\n",
    "   - Check that the symlink was created\n",
    "   - Verify models are in `/content/drive/MyDrive/ollama/models`\n",
    "\n",
    "### Performance Tips:\n",
    "\n",
    "- Use lightweight models for best performance\n",
    "- Keep Colab tab focused to prevent timeouts\n",
    "- Use Colab Pro for longer sessions and more resources\n",
    "- Test with simple prompts first\n",
    "\n",
    "---\n",
    "\n",
    "## üéâ Success!\n",
    "\n",
    "You now have Ollama running in Colab with:\n",
    "- ‚úÖ Persistent model storage via Google Drive\n",
    "- ‚úÖ Public web access via ngrok/Pinggy\n",
    "- ‚úÖ Integration ready for Goblin Assistant\n",
    "- ‚úÖ Cost-effective AI inference (free Colab)\n",
    "\n",
    "**Remember:** This is perfect for demos, testing, and development, but not production use due to session timeouts!\n",
    "\n",
    "**Next steps:**\n",
    "1. Copy your public URL\n",
    "2. Configure your Goblin Assistant backend\n",
    "3. Start chatting with your AI models! ü§ñ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
