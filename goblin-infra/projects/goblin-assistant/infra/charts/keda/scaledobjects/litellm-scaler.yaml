apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: litellm-scaler
  namespace: overmind-prod
  labels:
    app: litellm
    component: gateway
spec:
  scaleTargetRef:
    name: litellm
    kind: Deployment

  # Scaling configuration
  minReplicaCount: 2
  maxReplicaCount: 20
  pollingInterval: 30        # Check metrics every 30s
  cooldownPeriod: 300        # Wait 5min before scaling down

  # Advanced HPA configuration
  advanced:
    restoreToOriginalReplicaCount: false
    horizontalPodAutoscalerConfig:
      behavior:
        scaleDown:
          stabilizationWindowSeconds: 300
          policies:
          - type: Percent
            value: 50
            periodSeconds: 60
          - type: Pods
            value: 2
            periodSeconds: 60
          selectPolicy: Min
        scaleUp:
          stabilizationWindowSeconds: 0
          policies:
          - type: Percent
            value: 100
            periodSeconds: 30
          - type: Pods
            value: 4
            periodSeconds: 30
          selectPolicy: Max

  # Fallback if metrics unavailable
  fallback:
    failureThreshold: 3
    replicas: 5

  # Triggers for scaling
  triggers:
  # Primary: LLM request rate from Prometheus
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: llm_requests_per_second
      query: sum(rate(llm_request_total{service="litellm"}[1m]))
      threshold: "10"  # Scale up if > 10 req/sec
      activationThreshold: "1"  # Wake from 0 if > 1 req/sec

  # Secondary: CPU utilization
  - type: cpu
    metricType: Utilization
    metadata:
      value: "70"

  # Tertiary: Memory utilization
  - type: memory
    metricType: Utilization
    metadata:
      value: "80"

  # Quaternary: LLM cost control
  - type: prometheus
    metadata:
      serverAddress: http://prometheus:9090
      metricName: llm_cost_per_minute
      query: sum(rate(llm_cost_total{service="litellm"}[1m])) * 60
      threshold: "5"  # Scale if cost > $5/min
