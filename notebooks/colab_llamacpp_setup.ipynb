{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fuaadabdullah/forgemono/blob/main/notebooks/colab_llamacpp_setup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd190f54",
      "metadata": {
        "id": "dd190f54"
      },
      "source": [
        "# llama.cpp Server on Google Colab\n",
        "\n",
        "This notebook installs and runs `llama.cpp` (llama-server) on a Google Colab instance, stores the model on your Google Drive for persistence, and exposes a public endpoint using `ngrok`. It also provides optimization tips for Colab.\n",
        "\n",
        "**Notes:** Use the GPU runtime if you intend to use GPU features or large models, but for GGUF Q4 models CPU-only instances work well in Colab. The notebook downloads a model from Hugging Face ‚Äî choose a quantized variant (Q4_K, Q3_K_S, Q2_K) depending on your speed/accuracy needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "72c89fca",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72c89fca",
        "outputId": "834ab42e-62ca-417c-97e8-7c192949ffb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive mounted successfully!\n",
            "Model directory ready: /content/drive/MyDrive/llama_models\n"
          ]
        }
      ],
      "source": [
        "# 1) Mount Google Drive (persistent model storage)\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "USE_DRIVE = True  # Set to False to skip Drive and use local storage\n",
        "\n",
        "if USE_DRIVE:\n",
        "    try:\n",
        "        # Try to mount with force_remount to handle common issues\n",
        "        drive.mount(\"/content/drive\", force_remount=True)\n",
        "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
        "    except ValueError as e:\n",
        "        print(f\"‚ùå Drive mount failed: {e}\")\n",
        "        print(\"\\\\nüîß Troubleshooting steps:\")\n",
        "        print(\"1. Click the authentication link above\")\n",
        "        print(\"2. Sign in with your Google account\")\n",
        "        print(\"3. Click 'Allow' to grant access\")\n",
        "        print(\"4. Copy the authorization code back here\")\n",
        "        print(\"5. If it still fails, try:\")\n",
        "        print(\"   - Restart the runtime (Runtime ‚Üí Restart runtime)\")\n",
        "        print(\"   - Clear browser cache and try again\")\n",
        "        print(\"   - Use a different Google account\")\n",
        "        print(\"\\\\nüí° Falling back to local storage (/content/models/)\")\n",
        "        USE_DRIVE = False\n",
        "\n",
        "# Set up model directory\n",
        "if USE_DRIVE:\n",
        "    DRIVE_MODEL_DIR = \"/content/drive/MyDrive/llama_models\"\n",
        "else:\n",
        "    DRIVE_MODEL_DIR = \"/content/models\"\n",
        "\n",
        "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
        "print(f\"Model directory ready: {DRIVE_MODEL_DIR}\")\n",
        "\n",
        "if not USE_DRIVE:\n",
        "    print(\"‚ö†Ô∏è  Note: Models will be lost when the Colab session ends\")\n",
        "    print(\"   To persist models, fix the Drive mount issue above\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "f7fdca6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7fdca6d",
        "outputId": "2f80a0ed-4a58-4519-b5e1-8449568ab290"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Connecting to security.ubuntu.com (185.125.190.82)] [Connected to cloud.r-p\r                                                                               \rHit:2 https://cli.github.com/packages stable InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rHit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Waiting for headers] [Connected\r                                                                               \rHit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to developer.download\r                                                                               \rHit:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Waiting for headers] [Connected to ppa.launchpadconte\r                                                                               \rHit:7 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:8 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "build-essential is already the newest version (12.9ubuntu3).\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "git is already the newest version (1:2.34.1-1ubuntu1.15).\n",
            "unzip is already the newest version (6.0-26ubuntu3.2).\n",
            "wget is already the newest version (1.21.2-2ubuntu1.1).\n",
            "The following NEW packages will be installed:\n",
            "  pwgen\n",
            "0 upgraded, 1 newly installed, 0 to remove and 57 not upgraded.\n",
            "Need to get 17.4 kB of archives.\n",
            "After this operation, 53.2 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 pwgen amd64 2.08-2build1 [17.4 kB]\n",
            "Fetched 17.4 kB in 0s (79.2 kB/s)\n",
            "Selecting previously unselected package pwgen.\n",
            "(Reading database ... 121715 files and directories currently installed.)\n",
            "Preparing to unpack .../pwgen_2.08-2build1_amd64.deb ...\n",
            "Unpacking pwgen (2.08-2build1) ...\n",
            "Setting up pwgen (2.08-2build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Cloning into 'llama.cpp'...\n",
            "remote: Enumerating objects: 2162, done.\u001b[K\n",
            "remote: Counting objects: 100% (2162/2162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1657/1657), done.\u001b[K\n",
            "remote: Total 2162 (delta 486), reused 1553 (delta 433), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (2162/2162), 25.80 MiB | 19.74 MiB/s, done.\n",
            "Resolving deltas: 100% (486/486), done.\n",
            "/content/llama.cpp\n",
            "Makefile:6: *** Build system changed:\n",
            " The Makefile build has been replaced by CMake.\n",
            "\n",
            " For build instructions see:\n",
            " https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md\n",
            "\n",
            ".  Stop.\n",
            "/content\n",
            "llama.cpp built (server binary available in llama.cpp/build or llama.cpp/bin depending on build)\n"
          ]
        }
      ],
      "source": [
        "# 2) Install system packages and clone/build llama.cpp with the server target\n",
        "!apt-get update -y\n",
        "!apt-get install -y build-essential cmake git wget unzip pwgen\n",
        "\n",
        "# Build llama.cpp server\n",
        "!git clone https://github.com/ggerganov/llama.cpp.git --depth 1\n",
        "%cd llama.cpp\n",
        "!make -j$(nproc)\n",
        "%cd ..\n",
        "print(\n",
        "    \"llama.cpp built (server binary available in llama.cpp/build or llama.cpp/bin depending on build)\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "02150cb2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02150cb2",
        "outputId": "95504a04-d93e-4f0c-ad7c-64679905ac3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "deb https://ngrok-agent.s3.amazonaws.com buster main\n",
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 https://cli.github.com/packages stable InRelease\n",
            "Hit:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:6 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:7 https://ngrok-agent.s3.amazonaws.com buster InRelease\n",
            "Hit:8 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ngrok is already the newest version (3.33.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 57 not upgraded.\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n",
            "ngrok installed and authenticated; ready to create tunnels.\n"
          ]
        }
      ],
      "source": [
        "# 3) Install ngrok and forwarding helper (for public testing)\n",
        "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
        "!echo 'deb https://ngrok-agent.s3.amazonaws.com buster main' | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
        "!apt-get update -y && apt-get install -y ngrok\n",
        "\n",
        "# Configure ngrok with your auth token (replace with your token)\n",
        "!ngrok config add-authtoken 367SkOQHlBFw8AG1TsVNI0L9y46_3WJdDXhwNNLSJ1nn8JzCB\n",
        "\n",
        "print(\n",
        "    \"ngrok installed and authenticated; ready to create tunnels.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d0df62d2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0df62d2",
        "outputId": "283474db-57af-464a-ebfe-250ccd9e0c5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (0.20.3)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-1.1.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.2.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (6.0.3)\n",
            "Requirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (1.5.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.67.1)\n",
            "Requirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (0.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (4.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->huggingface-hub) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub) (0.16.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub) (1.3.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->huggingface-hub) (8.3.1)\n",
            "Downloading huggingface_hub-1.1.6-py3-none-any.whl (516 kB)\n",
            "Installing collected packages: huggingface-hub\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.20.3\n",
            "    Uninstalling huggingface-hub-0.20.3:\n",
            "      Successfully uninstalled huggingface-hub-0.20.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "transformers 4.57.2 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.1.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-1.1.6\n",
            "huggingface-hub installed; if you need private models, run `!huggingface-cli login` and paste your token.\n"
          ]
        }
      ],
      "source": [
        "# 4) (Optional) Install huggingface-cli to download private models if needed\n",
        "%pip install --upgrade pip huggingface-hub\n",
        "print(\n",
        "    \"huggingface-hub installed; if you need private models, run `!huggingface-cli login` and paste your token.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "18c8faa6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "18c8faa6",
        "outputId": "99151376-1bfc-4759-bdb8-4e1d651ff86b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Destination: /content/drive/MyDrive/llama_models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
            "‚úÖ Model already exists at /content/drive/MyDrive/llama_models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\n",
            "Starting llama.cpp server on port 8080...\n",
            "Command: ./llama.cpp/build/bin/llama-server --model /content/drive/MyDrive/llama_models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf --host 127.0.0.1 --port 8080 --threads 4 --ctx-size 2048 --n-gpu-layers 0 --api-key \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: './llama.cpp/build/bin/llama-server'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-118883342.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Start the server\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0mserver_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart_llama_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mserver_process\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-118883342.py\u001b[0m in \u001b[0;36mstart_llama_server\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;31m# Start server in background\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprocess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;31m# Wait a bit for server to start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                             encoding=encoding, errors=errors)\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001b[0m\u001b[1;32m   1027\u001b[0m                                 \u001b[0mpass_fds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcwd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m                                 \u001b[0mstartupinfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreationflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session, process_group)\u001b[0m\n\u001b[1;32m   1953\u001b[0m                         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrerror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1954\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0merr_filename\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1955\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1956\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './llama.cpp/build/bin/llama-server'"
          ]
        }
      ],
      "source": [
        "# 5) Download the model into your Google Drive folder\n",
        "# Example: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF (choose a quantization Q2/Q3/Q4 variant)\n",
        "MODEL_REPO = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
        "MODEL_FILENAME = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
        "dest_path = f\"{DRIVE_MODEL_DIR}/{MODEL_FILENAME}\"\n",
        "print(\"Destination:\", dest_path)\n",
        "\n",
        "# Check if model already exists\n",
        "if os.path.exists(dest_path):\n",
        "    print(f\"‚úÖ Model already exists at {dest_path}\")\n",
        "else:\n",
        "    print(f\"üì• Downloading model to {dest_path}\")\n",
        "    # Use huggingface-hub or wget to download the raw model.\n",
        "    # If model is public, wget works; for private models use huggingface-cli with a token.\n",
        "    !wget -O \"{dest_path}\" \"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILENAME}\"\n",
        "    print(f\"‚úÖ Model downloaded to {dest_path}\")\n",
        "\n",
        "# 6) Start the llama.cpp server\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "\n",
        "SERVER_PORT = 8080\n",
        "MODEL_PATH = dest_path\n",
        "\n",
        "def start_llama_server():\n",
        "    \"\"\"Start llama.cpp server in background\"\"\"\n",
        "    cmd = [\n",
        "        \"./llama.cpp/build/bin/llama-server\",\n",
        "        \"--model\", MODEL_PATH,\n",
        "        \"--host\", \"127.0.0.1\",\n",
        "        \"--port\", str(SERVER_PORT),\n",
        "        \"--threads\", \"4\",  # Adjust based on Colab CPU cores\n",
        "        \"--ctx-size\", \"2048\",  # Context window\n",
        "        \"--n-gpu-layers\", \"0\",  # Use 0 for CPU-only, or higher for GPU\n",
        "        \"--api-key\", \"\",  # No API key for local access\n",
        "    ]\n",
        "\n",
        "    print(f\"Starting llama.cpp server on port {SERVER_PORT}...\")\n",
        "    print(f\"Command: {' '.join(cmd)}\")\n",
        "\n",
        "    # Start server in background\n",
        "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "\n",
        "    # Wait a bit for server to start\n",
        "    time.sleep(5)\n",
        "\n",
        "    # Check if server is running\n",
        "    if process.poll() is None:\n",
        "        print(f\"‚úÖ Server started successfully on http://127.0.0.1:{SERVER_PORT}\")\n",
        "        return process\n",
        "    else:\n",
        "        stdout, stderr = process.communicate()\n",
        "        print(f\"‚ùå Server failed to start\")\n",
        "        print(f\"STDOUT: {stdout.decode()}\")\n",
        "        print(f\"STDERR: {stderr.decode()}\")\n",
        "        return None\n",
        "\n",
        "# Start the server\n",
        "server_process = start_llama_server()\n",
        "\n",
        "if server_process is None:\n",
        "    raise RuntimeError(\"Failed to start llama.cpp server\")\n",
        "\n",
        "# 9) Quick perf benchmark: run several prompts and measure latency/tokens/sec\n",
        "import time, requests\n",
        "\n",
        "url = \"http://127.0.0.1:8080/completions\"\n",
        "prompt = \"Benchmark: Provide a short helpful reply.\"\n",
        "N = 5\n",
        "times = []\n",
        "for i in range(N):\n",
        "    payload = {\"prompt\": prompt, \"max_tokens\": 64}\n",
        "    t0 = time.time()\n",
        "    r = requests.post(url, json=payload, timeout=30)\n",
        "    dt = time.time() - t0\n",
        "    times.append(dt)\n",
        "    print(f\"Run {i + 1} status={r.status_code} elapsed={dt:.2f}s\")\n",
        "\n",
        "print(\"Average elapsed\", sum(times) / len(times))\n",
        "print(\n",
        "    \"Tip: Tune --threads, try smaller quantizations (Q2/Q3), or increase --cache-ram to reduce latency.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d7f1a7f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0d7f1a7f",
        "outputId": "f7453049-869b-4797-b3d2-2298f032e986"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "üåê Public URL: https://thomasena-auxochromic-joziah.ngrok-free.dev\n",
            "üìù Copy this URL for your Goblin Assistant config\n"
          ]
        }
      ],
      "source": [
        "# 7) Setup ngrok tunnel for public access\n",
        "%pip install pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "\n",
        "# Kill any existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start tunnel to llama.cpp server\n",
        "tunnel = ngrok.connect(8080, \"http\")\n",
        "public_url = tunnel.public_url\n",
        "\n",
        "print(f\"üåê Public URL: {public_url}\")\n",
        "print(\"üìù Copy this URL for your Goblin Assistant config\")\n",
        "\n",
        "# Keep tunnel alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(1)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"Shutting down tunnel...\")\n",
        "    ngrok.kill()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41a9e0e5",
      "metadata": {
        "id": "41a9e0e5"
      },
      "outputs": [],
      "source": [
        "# 11) Generate Shareable Colab Link\n",
        "import urllib.parse\n",
        "import json\n",
        "import base64\n",
        "\n",
        "\n",
        "def generate_colab_link(ngrok_token=\"\", server_port=8080, model_repo=MODEL_REPO):\n",
        "    \"\"\"Generate a shareable Colab link with embedded configuration\"\"\"\n",
        "\n",
        "    # Get current notebook URL (this will be replaced when uploaded to Colab)\n",
        "    notebook_url = \"https://colab.research.google.com/drive/YOUR_NOTEBOOK_ID\"  # Replace with actual URL\n",
        "\n",
        "    # Create configuration\n",
        "    config = {\n",
        "        \"ngrok_token\": ngrok_token,\n",
        "        \"server_port\": server_port,\n",
        "        \"model_repo\": model_repo,\n",
        "        \"auto_start\": True,\n",
        "    }\n",
        "\n",
        "    # Encode config\n",
        "    config_json = json.dumps(config, separators=(\",\", \":\"))\n",
        "    config_b64 = base64.urlsafe_b64encode(config_json.encode()).decode()\n",
        "\n",
        "    # Build shareable URL\n",
        "    shareable_url = f\"{notebook_url}?config={config_b64}\"\n",
        "\n",
        "    print(\"üîó Shareable Colab Link:\")\n",
        "    print(shareable_url)\n",
        "    print(\"\\\\nüìã Instructions:\")\n",
        "    print(\n",
        "        \"1. Replace YOUR_NOTEBOOK_ID in the URL above with this notebook's actual Drive ID\"\n",
        "    )\n",
        "    print(\"2. Share this link with others\")\n",
        "    print(\"3. Recipients can open it directly in Colab with pre-configured settings\")\n",
        "\n",
        "    if ngrok_token:\n",
        "        print(\"\\\\n‚úÖ ngrok token included - tunnel will be created automatically\")\n",
        "    else:\n",
        "        print(\"\\\\n‚ö†Ô∏è  No ngrok token - users will need to add their own\")\n",
        "\n",
        "    return shareable_url\n",
        "\n",
        "\n",
        "# Generate link (add your ngrok token if you have one)\n",
        "YOUR_NGROK_TOKEN = \"\"  # Add your token here: ngrok config add-authtoken YOUR_TOKEN\n",
        "shareable_link = generate_colab_link(\n",
        "    ngrok_token=YOUR_NGROK_TOKEN, server_port=8080, model_repo=MODEL_REPO\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}