{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd190f54",
   "metadata": {},
   "source": [
    "# llama.cpp Server on Google Colab\n",
    "\n",
    "This notebook installs and runs `llama.cpp` (llama-server) on a Google Colab instance, stores the model on your Google Drive for persistence, and exposes a public endpoint using `ngrok`. It also provides optimization tips for Colab.\n",
    "\n",
    "**Notes:** Use the GPU runtime if you intend to use GPU features or large models, but for GGUF Q4 models CPU-only instances work well in Colab. The notebook downloads a model from Hugging Face ‚Äî choose a quantized variant (Q4_K, Q3_K_S, Q2_K) depending on your speed/accuracy needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c89fca",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel 'Python 3 (ipykernel)'. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details. Unable to get resolved server information for google.colab:colab:514c3d11-c283-459c-b2af-535d20a7cd00"
     ]
    }
   ],
   "source": [
    "# 1) Mount Google Drive (persistent model storage)\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "USE_DRIVE = True  # Set to False to skip Drive and use local storage\n",
    "\n",
    "if USE_DRIVE:\n",
    "    try:\n",
    "        # Try to mount with force_remount to handle common issues\n",
    "        drive.mount(\"/content/drive\", force_remount=True)\n",
    "        print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "    except ValueError as e:\n",
    "        print(f\"‚ùå Drive mount failed: {e}\")\n",
    "        print(\"\\\\nüîß Troubleshooting steps:\")\n",
    "        print(\"1. Click the authentication link above\")\n",
    "        print(\"2. Sign in with your Google account\")\n",
    "        print(\"3. Click 'Allow' to grant access\")\n",
    "        print(\"4. Copy the authorization code back here\")\n",
    "        print(\"5. If it still fails, try:\")\n",
    "        print(\"   - Restart the runtime (Runtime ‚Üí Restart runtime)\")\n",
    "        print(\"   - Clear browser cache and try again\")\n",
    "        print(\"   - Use a different Google account\")\n",
    "        print(\"\\\\nüí° Falling back to local storage (/content/models/)\")\n",
    "        USE_DRIVE = False\n",
    "\n",
    "# Set up model directory\n",
    "if USE_DRIVE:\n",
    "    DRIVE_MODEL_DIR = \"/content/drive/MyDrive/llama_models\"\n",
    "else:\n",
    "    DRIVE_MODEL_DIR = \"/content/models\"\n",
    "\n",
    "os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "print(f\"Model directory ready: {DRIVE_MODEL_DIR}\")\n",
    "\n",
    "if not USE_DRIVE:\n",
    "    print(\"‚ö†Ô∏è  Note: Models will be lost when the Colab session ends\")\n",
    "    print(\"   To persist models, fix the Drive mount issue above\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7fdca6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Install system packages and clone/build llama.cpp with the server target\n",
    "!apt-get update -y\n",
    "!apt-get install -y build-essential cmake git wget unzip pwgen\n",
    "\n",
    "# Build llama.cpp server\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git --depth 1\n",
    "%cd llama.cpp\n",
    "!make -j$(nproc)\n",
    "%cd ..\n",
    "print(\n",
    "    \"llama.cpp built (server binary available in llama.cpp/build or llama.cpp/bin depending on build)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02150cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Install ngrok and forwarding helper (for public testing)\n",
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
    "!echo 'deb https://ngrok-agent.s3.amazonaws.com buster main' | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
    "!apt-get update -y && apt-get install -y ngrok\n",
    "\n",
    "# If you have an ngrok auth token, set it via: ngrok config add-authtoken <YOUR-NGROK-AUTHTOKEN>\n",
    "print(\n",
    "    \"ngrok installed; run `!ngrok config add-authtoken <YOUR_TOKEN>` if you have one.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0df62d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) (Optional) Install huggingface-cli to download private models if needed\n",
    "%pip install --upgrade pip huggingface-hub\n",
    "print(\n",
    "    \"huggingface-hub installed; if you need private models, run `!huggingface-cli login` and paste your token.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c8faa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Download the model into your Google Drive folder\n",
    "# Example: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF (choose a quantization Q2/Q3/Q4 variant)\n",
    "MODEL_REPO = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "MODEL_FILENAME = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "dest_path = f\"{DRIVE_MODEL_DIR}/{MODEL_FILENAME}\"\n",
    "print(\"Destination:\", dest_path)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(dest_path):\n",
    "    print(f\"‚úÖ Model already exists at {dest_path}\")\n",
    "else:\n",
    "    print(f\"üì• Downloading model to {dest_path}\")\n",
    "    # Use huggingface-hub or wget to download the raw model.\n",
    "    # If model is public, wget works; for private models use huggingface-cli with a token.\n",
    "    !wget -O \"{dest_path}\" \"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILENAME}\"\n",
    "    print(f\"‚úÖ Model downloaded to {dest_path}\")\n",
    "\n",
    "# 6) Start the llama.cpp server\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "\n",
    "SERVER_PORT = 8080\n",
    "MODEL_PATH = dest_path\n",
    "\n",
    "def start_llama_server():\n",
    "    \"\"\"Start llama.cpp server in background\"\"\"\n",
    "    cmd = [\n",
    "        \"./llama.cpp/build/bin/llama-server\",\n",
    "        \"--model\", MODEL_PATH,\n",
    "        \"--host\", \"127.0.0.1\",\n",
    "        \"--port\", str(SERVER_PORT),\n",
    "        \"--threads\", \"4\",  # Adjust based on Colab CPU cores\n",
    "        \"--ctx-size\", \"2048\",  # Context window\n",
    "        \"--n-gpu-layers\", \"0\",  # Use 0 for CPU-only, or higher for GPU\n",
    "        \"--api-key\", \"\",  # No API key for local access\n",
    "    ]\n",
    "\n",
    "    print(f\"Starting llama.cpp server on port {SERVER_PORT}...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "    # Start server in background\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Wait a bit for server to start\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Check if server is running\n",
    "    if process.poll() is None:\n",
    "        print(f\"‚úÖ Server started successfully on http://127.0.0.1:{SERVER_PORT}\")\n",
    "        return process\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "        print(f\"‚ùå Server failed to start\")\n",
    "        print(f\"STDOUT: {stdout.decode()}\")\n",
    "        print(f\"STDERR: {stderr.decode()}\")\n",
    "        return None\n",
    "\n",
    "# Start the server\n",
    "server_process = start_llama_server()\n",
    "\n",
    "if server_process is None:\n",
    "    raise RuntimeError(\"Failed to start llama.cpp server\")\n",
    "<VSCode.Cell id=\"#VSC-e8941fff\" language=\"python\">\n",
    "# 9) Quick perf benchmark: run several prompts and measure latency/tokens/sec\n",
    "import time, requests\n",
    "\n",
    "url = \"http://127.0.0.1:8080/completions\"\n",
    "prompt = \"Benchmark: Provide a short helpful reply.\"\n",
    "N = 5\n",
    "times = []\n",
    "for i in range(N):\n",
    "    payload = {\"prompt\": prompt, \"max_tokens\": 64}\n",
    "    t0 = time.time()\n",
    "    r = requests.post(url, json=payload, timeout=30)\n",
    "    dt = time.time() - t0\n",
    "    times.append(dt)\n",
    "    print(f\"Run {i + 1} status={r.status_code} elapsed={dt:.2f}s\")\n",
    "\n",
    "print(\"Average elapsed\", sum(times) / len(times))\n",
    "print(\n",
    "    \"Tip: Tune --threads, try smaller quantizations (Q2/Q3), or increase --cache-ram to reduce latency.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7f1a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Setup ngrok tunnel for public access\n",
    "%pip install pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "import time\n",
    "\n",
    "# Kill any existing tunnels\n",
    "ngrok.kill()\n",
    "\n",
    "# Start tunnel to llama.cpp server\n",
    "tunnel = ngrok.connect(8080, \"http\")\n",
    "public_url = tunnel.public_url\n",
    "\n",
    "print(f\"üåê Public URL: {public_url}\")\n",
    "print(\"üìù Copy this URL for your Goblin Assistant config\")\n",
    "\n",
    "# Keep tunnel alive\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Shutting down tunnel...\")\n",
    "    ngrok.kill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a9e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Generate Shareable Colab Link\n",
    "import urllib.parse\n",
    "import json\n",
    "import base64\n",
    "\n",
    "\n",
    "def generate_colab_link(ngrok_token=\"\", server_port=8080, model_repo=MODEL_REPO):\n",
    "    \"\"\"Generate a shareable Colab link with embedded configuration\"\"\"\n",
    "\n",
    "    # Get current notebook URL (this will be replaced when uploaded to Colab)\n",
    "    notebook_url = \"https://colab.research.google.com/drive/YOUR_NOTEBOOK_ID\"  # Replace with actual URL\n",
    "\n",
    "    # Create configuration\n",
    "    config = {\n",
    "        \"ngrok_token\": ngrok_token,\n",
    "        \"server_port\": server_port,\n",
    "        \"model_repo\": model_repo,\n",
    "        \"auto_start\": True,\n",
    "    }\n",
    "\n",
    "    # Encode config\n",
    "    config_json = json.dumps(config, separators=(\",\", \":\"))\n",
    "    config_b64 = base64.urlsafe_b64encode(config_json.encode()).decode()\n",
    "\n",
    "    # Build shareable URL\n",
    "    shareable_url = f\"{notebook_url}?config={config_b64}\"\n",
    "\n",
    "    print(\"üîó Shareable Colab Link:\")\n",
    "    print(shareable_url)\n",
    "    print(\"\\\\nüìã Instructions:\")\n",
    "    print(\n",
    "        \"1. Replace YOUR_NOTEBOOK_ID in the URL above with this notebook's actual Drive ID\"\n",
    "    )\n",
    "    print(\"2. Share this link with others\")\n",
    "    print(\"3. Recipients can open it directly in Colab with pre-configured settings\")\n",
    "\n",
    "    if ngrok_token:\n",
    "        print(\"\\\\n‚úÖ ngrok token included - tunnel will be created automatically\")\n",
    "    else:\n",
    "        print(\"\\\\n‚ö†Ô∏è  No ngrok token - users will need to add their own\")\n",
    "\n",
    "    return shareable_url\n",
    "\n",
    "\n",
    "# Generate link (add your ngrok token if you have one)\n",
    "YOUR_NGROK_TOKEN = \"\"  # Add your token here: ngrok config add-authtoken YOUR_TOKEN\n",
    "shareable_link = generate_colab_link(\n",
    "    ngrok_token=YOUR_NGROK_TOKEN, server_port=8080, model_repo=MODEL_REPO\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
