# Quick Deployment Reference

## Deployment URLs

| Service | URL | Purpose |
|---------|-----|---------|
| Frontend (Vercel) | https://goblin-assistant.vercel.app | React UI, Static Assets |
| Backend (Render) | https://goblin-assistant-backend.onrender.com | FastAPI Server, Database |
| LLM Server (Kamatera) | http://45.61.60.3:8002 | Local Ollama Models |

## Quick Deploy Commands

### Frontend to Vercel
```bash
cd apps/goblin-assistant
./deploy-vercel.sh
# Or manually:
npm run build
vercel --prod
```

### Backend to Render
```bash
cd apps/goblin-assistant
./deploy-render-prep.sh
# Then push to GitHub - Render auto-deploys
git push origin main
```

### Kamatera LLM Server
```bash
# SSH into server
ssh root@45.61.60.3

# Check Ollama status
systemctl status ollama

# Restart if needed
systemctl restart ollama

# Test API
curl http://45.61.60.3:8002/health
```

## Essential Environment Variables

### Vercel (Frontend)
```bash
VITE_API_URL=https://goblin-assistant-backend.onrender.com
VITE_FRONTEND_URL=https://goblin-assistant.vercel.app
VITE_GOOGLE_CLIENT_ID=<your-google-client-id>
```

### Render (Backend)
```bash
DATABASE_URL=<from-render-postgres>
LOCAL_LLM_PROXY_URL=http://45.61.60.3:8002
LOCAL_LLM_API_KEY=<kamatera-api-key>
FRONTEND_URL=https://goblin-assistant.vercel.app
JWT_SECRET_KEY=<auto-generated>
OPENAI_API_KEY=<your-key>
ANTHROPIC_API_KEY=<your-key>
```

### Kamatera (LLM Server)
```bash
# No special env vars needed
# Ollama runs as system service
# API proxy handles authentication
```

## Health Check Endpoints

```bash
# Frontend
curl https://goblin-assistant.vercel.app

# Backend
curl https://goblin-assistant-backend.onrender.com/health

# LLM Server
curl http://45.61.60.3:8002/health

# Test full chain
curl -X POST https://goblin-assistant-backend.onrender.com/api/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <your-jwt-token>" \
  -d '{
    "model": "gemma:2b",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

## Rollback Procedures

### Frontend (Vercel)
```bash
# Via CLI
vercel rollback <deployment-url>

# Via Dashboard
# Deployments → Select previous → Promote to Production
```

### Backend (Render)
```bash
# Via Dashboard only
# Services → Select service → Deploys → Redeploy previous
```

### LLM Server (Kamatera)
```bash
# Restart Ollama
systemctl restart ollama

# Reload specific model
ollama pull gemma:2b --force
```

## Common Issues & Fixes

### Issue: Frontend can't reach backend
**Fix**: Check CORS settings in backend and verify VITE_API_URL

### Issue: Backend timeout calling LLM
**Fix**: Check Kamatera server is running and firewall allows connections

### Issue: Models loading slowly
**Fix**: Increase timeout in backend or warm up models on Kamatera

### Issue: Database connection error
**Fix**: Verify DATABASE_URL in Render and PostgreSQL service is running

## Monitoring

- **Vercel**: https://vercel.com/dashboard
- **Render**: https://dashboard.render.com
- **Kamatera**: SSH access + `systemctl status ollama`

## Cost Summary

| Service | Plan | Monthly Cost |
|---------|------|--------------|
| Vercel | Hobby | $0 |
| Render (Web Service) | Starter | $7 |
| Render (PostgreSQL) | Starter | $7 |
| Kamatera (VPS) | Custom | $20-50 |
| **Total** | | **$34-64** |

## Support Resources

- **Full Guide**: See `DEPLOYMENT_ARCHITECTURE.md`
- **GoblinOS Docs**: `/GoblinOS/docs/`
- **Vercel Docs**: https://vercel.com/docs
- **Render Docs**: https://render.com/docs
