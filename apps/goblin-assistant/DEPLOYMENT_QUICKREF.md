# Quick Deployment Reference

## Deployment URLs

| Service | URL | Purpose |
|---------|-----|---------|
| Frontend (Vercel) | <https://goblin-assistant.vercel.app> | React UI, Static Assets |
| Backend (Fly.io) | <https://goblin-assistant.fly.dev> | FastAPI Server, Database |
| LLM Server (Kamatera) | <http://45.61.60.3:8002> | Local Ollama Models |

## Quick Deploy Commands

### Frontend to Vercel

```bash
cd apps/goblin-assistant
./deploy-vercel.sh
# Or manually:
npm run build
vercel --prod
```

### Backend to Fly.io
```bash

cd apps/goblin-assistant
./deploy-fly.sh

# Or manually:
fly deploy
```

### Kamatera LLM Server

```bash
# SSH into server
ssh root@45.61.60.3

# Check Ollama status
systemctl status ollama

# Restart if needed
systemctl restart ollama

# Test API
curl http://45.61.60.3:8002/health
```

## Essential Environment Variables

### Vercel (Frontend)
```bash

VITE_API_URL=<https://goblin-assistant.fly.dev>
VITE_FRONTEND_URL=<https://goblin-assistant.vercel.app>
VITE_GOOGLE_CLIENT_ID=<your-google-client-id>
```

### Fly.io (Backend)

```bash
DATABASE_URL=<from-supabase>
LOCAL_LLM_PROXY_URL=http://45.61.60.3:8002
LOCAL_LLM_API_KEY=<kamatera-api-key>
FRONTEND_URL=https://goblin-assistant.vercel.app
JWT_SECRET_KEY=<auto-generated>
OPENAI_API_KEY=<your-key>
ANTHROPIC_API_KEY=<your-key>
```

### Kamatera (LLM Server)
```bash

# No special env vars needed

# Ollama runs as system service

# API proxy handles authentication
```

## Health Check Endpoints

```bash
# Frontend
curl https://goblin-assistant.vercel.app

# Backend
curl https://goblin-assistant.fly.dev/health

# LLM Server
curl http://45.61.60.3:8002/health

# Test full chain
curl -X POST https://goblin-assistant.fly.dev/api/chat/completions \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer <your-jwt-token>" \
  -d '{
    "model": "gemma:2b",
    "messages": [{"role": "user", "content": "Hello"}]
  }'
```

## Rollback Procedures

### Frontend (Vercel)
```bash

# Via CLI
vercel rollback <deployment-url>

# Via Dashboard

# Deployments → Select previous → Promote to Production
```

### Backend (Fly.io)

```bash
# Via CLI
fly deploy --image <previous-image-id>

# Via Dashboard
# Apps → Select app → Activity → Scale previous deployment
```

### LLM Server (Kamatera)
```bash

# Restart Ollama
systemctl restart ollama

# Reload specific model
ollama pull gemma:2b --force
```

## Common Issues & Fixes

### Issue: Frontend can't reach backend
**Fix**: Check CORS settings in backend and verify VITE_API_URL

### Issue: Backend timeout calling LLM
**Fix**: Check Kamatera server is running and firewall allows connections

### Issue: Models loading slowly
**Fix**: Increase timeout in backend or warm up models on Kamatera

### Issue: Database connection error
**Fix**: Verify DATABASE_URL in Fly.io and Supabase is running

## Monitoring

- **Vercel**: <https://vercel.com/dashboard>
- **Fly.io**: <https://fly.io/dashboard>
- **Kamatera**: SSH access + `systemctl status ollama`

## Cost Summary

| Service | Plan | Monthly Cost |
|---------|------|--------------|
| Vercel | Hobby | $0 |
| Fly.io (Web Service) | Free | $0 |
| Supabase (PostgreSQL) | Free | $0 |
| Kamatera (VPS) | Custom | $20-50 |
| **Total** | | **$20-50** |

## Support Resources

- **Full Guide**: See `DEPLOYMENT_ARCHITECTURE.md`
- **GoblinOS Docs**: `/GoblinOS/docs/`
- **Vercel Docs**: <https://vercel.com/docs>
- **Fly.io Docs**: <https://fly.io/docs>
