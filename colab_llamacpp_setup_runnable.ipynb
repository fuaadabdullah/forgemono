{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e13d9c6e",
   "metadata": {},
   "source": [
    "# llama.cpp Server on Google Colab\n",
    "\n",
    "This notebook installs and runs `llama.cpp` (llama-server) on a Google Colab instance, stores the model on your Google Drive for persistence, and exposes a public endpoint using `ngrok`. It also provides optimization tips for Colab.\n",
    "\n",
    "**Notes:** Use the GPU runtime if you intend to use GPU features or large models, but for GGUF Q4 models CPU-only instances work well in Colab. The notebook downloads a model from Hugging Face ‚Äî choose a quantized variant (Q4_K, Q3_K_S, Q2_K) depending on your speed/accuracy needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd744f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: Unmount Drive (useful to re-auth and switch accounts)\n",
    "from google.colab import drive\n",
    "\n",
    "\n",
    "def unmount_drive():\n",
    "    \"\"\"Unmount Google Drive and provide guidance to re-auth.\"\"\"\n",
    "    drive.flush_and_unmount()\n",
    "    print(\n",
    "        \"Drive unmounted. Re-run the mount cell and sign in with the desired Google account.\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Usage: run `unmount_drive()` then re-run the mount cell to authenticate as a different Google user\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0d4c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Mount Google Drive (persistent model storage)\n",
    "from google.colab import drive\n",
    "import os\n",
    "import time\n",
    "\n",
    "USE_DRIVE = True  # Set to False to skip Drive and use local storage\n",
    "# Expected Google account to use for Drive (change if needed)\n",
    "EXPECTED_DRIVE_EMAIL = \"fuaadabdullah@gmail.com\"\n",
    "\n",
    "DRIVE_ROOT = None\n",
    "DRIVE_MODEL_DIR = None\n",
    "\n",
    "\n",
    "def mount_google_drive(max_retries=3):\n",
    "    \"\"\"Mount Google Drive with retry logic and set DRIVE_MODEL_DIR.\"\"\"\n",
    "    global DRIVE_ROOT, DRIVE_MODEL_DIR\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            print(\n",
    "                f\"üîÑ Attempting to mount Google Drive (attempt {attempt + 1}/{max_retries})...\"\n",
    "            )\n",
    "            drive.mount(\"/content/drive\", force_remount=True)\n",
    "            print(\"‚úÖ Google Drive mounted successfully!\")\n",
    "\n",
    "            # Detect canonical paths\n",
    "            if os.path.exists(\"/content/drive/MyDrive\"):\n",
    "                DRIVE_ROOT = \"/content/drive/MyDrive\"\n",
    "            elif os.path.exists(\"/content/drive/My Drive\"):\n",
    "                DRIVE_ROOT = \"/content/drive/My Drive\"\n",
    "                # create /content/drive/MyDrive symlink for compatibility\n",
    "                try:\n",
    "                    if not os.path.exists(\"/content/drive/MyDrive\"):\n",
    "                        os.symlink(\"/content/drive/My Drive\", \"/content/drive/MyDrive\")\n",
    "                        print(\n",
    "                            \"üîó Created symlink /content/drive/MyDrive -> /content/drive/My Drive\"\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ö†Ô∏è Could not create symlink: {e}\")\n",
    "            else:\n",
    "                # mounted but expected subpaths missing\n",
    "                DRIVE_ROOT = \"/content/drive\"\n",
    "                print(\n",
    "                    \"‚ö†Ô∏è Drive mounted but MyDrive/My Drive not found under /content/drive\"\n",
    "                )\n",
    "\n",
    "            # Set model dir\n",
    "            DRIVE_MODEL_DIR = (\n",
    "                os.path.join(DRIVE_ROOT, \"llama_models\")\n",
    "                if DRIVE_ROOT\n",
    "                else \"/content/models\"\n",
    "            )\n",
    "            os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "            print(f\"üìÅ Using model directory: {DRIVE_MODEL_DIR}\")\n",
    "\n",
    "            # Verify signed-in email if possible\n",
    "            try:\n",
    "                from google.colab import auth\n",
    "\n",
    "                auth.authenticate_user()\n",
    "                from googleapiclient.discovery import build\n",
    "\n",
    "                oauth2 = build(\"oauth2\", \"v2\")\n",
    "                userinfo = oauth2.userinfo().get().execute()\n",
    "                signed_in_email = userinfo.get(\"email\")\n",
    "                print(f\"--- Signed-in account: {signed_in_email}\")\n",
    "                if (\n",
    "                    signed_in_email\n",
    "                    and signed_in_email.lower() != EXPECTED_DRIVE_EMAIL.lower()\n",
    "                ):\n",
    "                    print(\"‚ö†Ô∏è Signed-in account does not match expected account\")\n",
    "                    print(f\"    Expected: {EXPECTED_DRIVE_EMAIL}\")\n",
    "                    print(f\"    Found:    {signed_in_email}\")\n",
    "                    print(\n",
    "                        \"To fix: run drive.flush_and_unmount() then re-run this cell and sign in with the correct account.\"\n",
    "                    )\n",
    "                    return False\n",
    "            except Exception as e:\n",
    "                print(f\"‚ö†Ô∏è Could not verify Google account: {e}\")\n",
    "\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Drive mount attempt {attempt + 1} failed: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                print(\"‚è≥ Waiting 3 seconds before retry...\")\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                print(\"\\nüîß Troubleshooting steps:\")\n",
    "                print(\"1. Click the authentication link above and sign in\")\n",
    "                print(\n",
    "                    \"2. Use an incognito browser window and sign into the desired account\"\n",
    "                )\n",
    "                print(\n",
    "                    \"3. If needed, run: drive.flush_and_unmount() then re-run this cell\"\n",
    "                )\n",
    "                return False\n",
    "    return False\n",
    "\n",
    "\n",
    "if USE_DRIVE:\n",
    "    drive_mounted = mount_google_drive()\n",
    "    if not drive_mounted:\n",
    "        print(\"\\nüí° Falling back to local storage (/content/models/)\")\n",
    "        USE_DRIVE = False\n",
    "        DRIVE_MODEL_DIR = \"/content/models\"\n",
    "        os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "else:\n",
    "    DRIVE_MODEL_DIR = \"/content/models\"\n",
    "    os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)\n",
    "    print(\"‚ÑπÔ∏è  Using local storage for models (won't persist across sessions)\")\n",
    "\n",
    "print(f\"FINAL: USE_DRIVE={USE_DRIVE}, DRIVE_MODEL_DIR={DRIVE_MODEL_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4646d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Install system packages and clone/build llama.cpp with the server target\n",
    "!apt-get update -y\n",
    "!apt-get install -y build-essential cmake git wget unzip pwgen\n",
    "\n",
    "# Clone and build llama.cpp\n",
    "!git clone https://github.com/ggerganov/llama.cpp.git --depth 1\n",
    "%cd llama.cpp\n",
    "\n",
    "# Build with verbose output to see what's happening\n",
    "!make -j$(nproc) 2>&1 | head -50\n",
    "\n",
    "# Check if build was successful\n",
    "!ls -la build/bin/ 2>/dev/null || echo \"build/bin/ not found\"\n",
    "!ls -la llama-server 2>/dev/null || echo \"llama-server not found in root\"\n",
    "!ls -la build/llama-server 2>/dev/null || echo \"build/llama-server not found\"\n",
    "\n",
    "%cd ..\n",
    "print(\"llama.cpp build completed - check output above for any errors\")\n",
    "print(\n",
    "    \"If build failed, you may need to install additional dependencies or check the logs\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dcc82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5) Verify llama.cpp build and troubleshoot if needed\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def check_llama_build():\n",
    "    \"\"\"Check if llama.cpp was built successfully\"\"\"\n",
    "    print(\"üîç Checking llama.cpp build status...\")\n",
    "\n",
    "    # Check for binary in various locations\n",
    "    binary_locations = [\n",
    "        \"./llama.cpp/build/bin/llama-server\",\n",
    "        \"./llama.cpp/bin/llama-server\",\n",
    "        \"./llama.cpp/llama-server\",\n",
    "        \"./llama.cpp/build/llama-server\",\n",
    "    ]\n",
    "\n",
    "    found_binary = None\n",
    "    for location in binary_locations:\n",
    "        if os.path.exists(location):\n",
    "            found_binary = location\n",
    "            print(f\"‚úÖ Found llama-server binary at: {location}\")\n",
    "            break\n",
    "\n",
    "    if not found_binary:\n",
    "        print(\"‚ùå llama-server binary not found!\")\n",
    "        print(\"\\nüîß Troubleshooting steps:\")\n",
    "\n",
    "        # Check if we're in the right directory\n",
    "        if not os.path.exists(\"./llama.cpp\"):\n",
    "            print(\"1. llama.cpp directory not found - cell 2 may have failed\")\n",
    "            print(\"   ‚Üí Re-run cell 2\")\n",
    "            return False\n",
    "\n",
    "        # Check build directory\n",
    "        if not os.path.exists(\"./llama.cpp/build\"):\n",
    "            print(\"2. Build directory doesn't exist - build may have failed\")\n",
    "            print(\"   ‚Üí Check cell 2 output for errors\")\n",
    "            print(\"   ‚Üí Try: cd llama.cpp && make clean && make -j$(nproc)\")\n",
    "            return False\n",
    "\n",
    "        # Check for common build issues\n",
    "        print(\"3. Checking for common build issues...\")\n",
    "\n",
    "        # Check if cmake is available\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"cmake\", \"--version\"], capture_output=True, text=True, timeout=5\n",
    "            )\n",
    "            if result.returncode != 0:\n",
    "                print(\"   - CMake may not be available\")\n",
    "        except:\n",
    "            print(\"   - CMake may not be available\")\n",
    "\n",
    "        # Check if git clone was complete\n",
    "        if not os.path.exists(\"./llama.cpp/Makefile\"):\n",
    "            print(\"   - Makefile not found - git clone may have failed\")\n",
    "            print(\n",
    "                \"   ‚Üí Re-run: !git clone https://github.com/ggerganov/llama.cpp.git --depth 1\"\n",
    "            )\n",
    "\n",
    "        print(\"4. Try manual build:\")\n",
    "        print(\n",
    "            \"   !cd llama.cpp && make clean && make -j2  # Use fewer cores if memory issues\"\n",
    "        )\n",
    "\n",
    "        return False\n",
    "\n",
    "    # Test if binary is executable\n",
    "    try:\n",
    "        result = subprocess.run(\n",
    "            [found_binary, \"--version\"], capture_output=True, text=True, timeout=10\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"‚úÖ Binary is executable: {result.stdout.strip()}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå Binary not executable: {result.stderr.strip()}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error testing binary: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Run the check\n",
    "build_ok = check_llama_build()\n",
    "\n",
    "if not build_ok:\n",
    "    print(\"\\n‚ö†Ô∏è  Build issues detected. Please fix before proceeding to cell 6.\")\n",
    "    print(\"Once fixed, re-run this cell to verify.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Build looks good! Proceed to cell 6.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f25722f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Install ngrok and forwarding helper (for public testing)\n",
    "!curl -s https://ngrok-agent.s3.amazonaws.com/ngrok.asc | sudo tee /etc/apt/trusted.gpg.d/ngrok.asc >/dev/null\n",
    "!echo 'deb https://ngrok-agent.s3.amazonaws.com buster main' | sudo tee /etc/apt/sources.list.d/ngrok.list\n",
    "!apt-get update -y && apt-get install -y ngrok\n",
    "\n",
    "# Set ngrok auth token\n",
    "!ngrok config add-authtoken 367SkOQHlBFw8AG1TsVNI0L9y46_3WJdDXhwNNLSJ1nn8JzCB\n",
    "print(\"ngrok installed and authenticated successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41c85a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) (Optional) Install huggingface-cli to download private models if needed\n",
    "!pip install --upgrade pip huggingface-hub==0.20.3\n",
    "print(\n",
    "    \"huggingface-hub installed; if you need private models, run `!huggingface-cli login` and paste your token.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c48916e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Download the model into your Google Drive folder\n",
    "# Example: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF (choose a quantization Q2/Q3/Q4 variant)\n",
    "MODEL_REPO = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "MODEL_FILENAME = \"tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n",
    "dest_path = f\"{DRIVE_MODEL_DIR}/{MODEL_FILENAME}\"\n",
    "print(\"Destination:\", dest_path)\n",
    "\n",
    "# Check if model already exists\n",
    "if os.path.exists(dest_path):\n",
    "    print(f\"‚úÖ Model already exists at {dest_path}\")\n",
    "else:\n",
    "    print(f\"üì• Downloading model to {dest_path}\")\n",
    "    # Use huggingface-hub or wget to download the raw model.\n",
    "    # If model is public, wget works; for private models use huggingface-cli with a token.\n",
    "    !wget -O \"{dest_path}\" \"https://huggingface.co/{MODEL_REPO}/resolve/main/{MODEL_FILENAME}\"\n",
    "    print(f\"‚úÖ Model downloaded to {dest_path}\")\n",
    "\n",
    "# Verify the model file\n",
    "if os.path.exists(dest_path):\n",
    "    size_mb = os.path.getsize(dest_path) / (1024 * 1024)\n",
    "    print(f\"‚úÖ Model verified: {MODEL_FILENAME} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    print(f\"‚ùå Model download failed: {dest_path}\")\n",
    "    raise FileNotFoundError(f\"Model not found at {dest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aabc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Start the llama.cpp server\n",
    "import subprocess\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "\n",
    "SERVER_PORT = 8080\n",
    "MODEL_PATH = dest_path\n",
    "\n",
    "\n",
    "def find_llama_server():\n",
    "    \"\"\"Find the llama-server binary in possible locations\"\"\"\n",
    "    possible_paths = [\n",
    "        \"./llama.cpp/build/bin/llama-server\",\n",
    "        \"./llama.cpp/bin/llama-server\",\n",
    "        \"./llama.cpp/llama-server\",\n",
    "        \"./llama.cpp/build/llama-server\",\n",
    "    ]\n",
    "\n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"‚úÖ Found llama-server at: {path}\")\n",
    "            return path\n",
    "\n",
    "    # Check if it exists anywhere in the llama.cpp directory\n",
    "    import glob\n",
    "\n",
    "    llama_server_files = glob.glob(\"./llama.cpp/**/llama-server\", recursive=True)\n",
    "    if llama_server_files:\n",
    "        path = llama_server_files[0]\n",
    "        print(f\"‚úÖ Found llama-server at: {path}\")\n",
    "        return path\n",
    "\n",
    "    print(\"‚ùå llama-server binary not found in expected locations\")\n",
    "    print(\"Checking build directory contents...\")\n",
    "\n",
    "    # Debug: show what's in the build directory\n",
    "    if os.path.exists(\"./llama.cpp/build\"):\n",
    "        print(\"Contents of ./llama.cpp/build/:\")\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ls\", \"-la\", \"./llama.cpp/build/\"], capture_output=True, text=True\n",
    "            )\n",
    "            print(result.stdout)\n",
    "        except:\n",
    "            print(\"Could not list build directory\")\n",
    "\n",
    "    if os.path.exists(\"./llama.cpp/build/bin\"):\n",
    "        print(\"Contents of ./llama.cpp/build/bin/:\")\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"ls\", \"-la\", \"./llama.cpp/build/bin/\"], capture_output=True, text=True\n",
    "            )\n",
    "            print(result.stdout)\n",
    "        except:\n",
    "            print(\"Could not list build/bin directory\")\n",
    "\n",
    "    print(\n",
    "        \"\\nüí° Build may have failed. Try running cell 2 again or check the build output above.\"\n",
    "    )\n",
    "    return None\n",
    "\n",
    "\n",
    "def start_llama_server():\n",
    "    \"\"\"Start llama.cpp server in background\"\"\"\n",
    "\n",
    "    # Find the binary first\n",
    "    server_binary = find_llama_server()\n",
    "    if not server_binary:\n",
    "        return None\n",
    "\n",
    "    cmd = [\n",
    "        server_binary,\n",
    "        \"--model\",\n",
    "        MODEL_PATH,\n",
    "        \"--host\",\n",
    "        \"127.0.0.1\",\n",
    "        \"--port\",\n",
    "        str(SERVER_PORT),\n",
    "        \"--threads\",\n",
    "        \"4\",  # Adjust based on Colab CPU cores\n",
    "        \"--ctx-size\",\n",
    "        \"2048\",  # Context window\n",
    "        \"--n-gpu-layers\",\n",
    "        \"0\",  # Use 0 for CPU-only, or higher for GPU\n",
    "        \"--api-key\",\n",
    "        \"\",  # No API key for local access\n",
    "    ]\n",
    "\n",
    "    print(f\"Starting llama.cpp server on port {SERVER_PORT}...\")\n",
    "    print(f\"Command: {' '.join(cmd)}\")\n",
    "\n",
    "    # Start server in background\n",
    "    process = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "\n",
    "    # Wait a bit for server to start\n",
    "    time.sleep(5)\n",
    "\n",
    "    # Check if server is running\n",
    "    if process.poll() is None:\n",
    "        print(f\"‚úÖ Server started successfully on http://127.0.0.1:{SERVER_PORT}\")\n",
    "        return process\n",
    "    else:\n",
    "        stdout, stderr = process.communicate()\n",
    "        print(f\"‚ùå Server failed to start\")\n",
    "        print(f\"STDOUT: {stdout.decode()}\")\n",
    "        print(f\"STDERR: {stderr.decode()}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Start the server\n",
    "server_process = start_llama_server()\n",
    "\n",
    "if server_process is None:\n",
    "    raise RuntimeError(\n",
    "        \"Failed to start llama.cpp server - check build output in cell 2\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce11c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Set up ngrok tunnel for public access\n",
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "\n",
    "def setup_ngrok_tunnel(port=SERVER_PORT):\n",
    "    \"\"\"Set up ngrok tunnel to expose the local server\"\"\"\n",
    "\n",
    "    # Check if ngrok is installed\n",
    "    try:\n",
    "        result = subprocess.run([\"ngrok\", \"version\"], capture_output=True, text=True)\n",
    "        if result.returncode != 0:\n",
    "            print(\"‚ùå ngrok not found. Please run cell 3 first to install ngrok.\")\n",
    "            return None\n",
    "    except FileNotFoundError:\n",
    "        print(\"‚ùå ngrok not found. Please run cell 3 first to install ngrok.\")\n",
    "        return None\n",
    "\n",
    "    print(\"Starting ngrok tunnel...\")\n",
    "\n",
    "    # Start ngrok in background\n",
    "    ngrok_cmd = [\"ngrok\", \"http\", str(port)]\n",
    "    ngrok_process = subprocess.Popen(\n",
    "        ngrok_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE\n",
    "    )\n",
    "\n",
    "    # Wait for ngrok to start\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Get the public URL\n",
    "    try:\n",
    "        response = requests.get(\"http://127.0.0.1:4040/api/tunnels\", timeout=5)\n",
    "        if response.status_code == 200:\n",
    "            tunnels = response.json()[\"tunnels\"]\n",
    "            if tunnels:\n",
    "                public_url = tunnels[0][\"public_url\"]\n",
    "                print(f\"‚úÖ ngrok tunnel active: {public_url}\")\n",
    "                print(f\"üîó Use this URL in your Goblin Assistant configuration\")\n",
    "                return public_url\n",
    "            else:\n",
    "                print(\"‚ùå No tunnels found\")\n",
    "        else:\n",
    "            print(f\"‚ùå Failed to get tunnel info: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error getting ngrok URL: {e}\")\n",
    "        print(\"üí° Make sure ngrok is running. Check the output above.\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "# Set up the tunnel\n",
    "ngrok_url = setup_ngrok_tunnel(SERVER_PORT)\n",
    "\n",
    "if ngrok_url:\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"üéâ INTEGRATION COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Public endpoint: {ngrok_url}\")\n",
    "    print(\"\\nüìã Next steps:\")\n",
    "    print(\"1. Copy the ngrok URL above\")\n",
    "    print(\"2. Run this command on your local machine:\")\n",
    "    print(\n",
    "        f\"   python3 setup_colab_integration.py --provider llamacpp --colab-url {ngrok_url} --auto-test\"\n",
    "    )\n",
    "    print(\"3. Test the integration:\")\n",
    "    print(\n",
    "        \"   python3 scripts/test_goblin_colab_integration.py --backend-url http://localhost:8000\"\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n‚ùå Failed to set up ngrok tunnel\")\n",
    "    print(\"üí° Troubleshooting:\")\n",
    "    print(\"1. Make sure you have an ngrok account and auth token\")\n",
    "    print(\"2. Run: !ngrok config add-authtoken YOUR_TOKEN\")\n",
    "    print(\"3. Restart this cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0f89ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Quick perf benchmark: run several prompts and measure latency/tokens/sec\n",
    "import time\n",
    "import requests\n",
    "\n",
    "url = \"http://127.0.0.1:8080/completions\"\n",
    "prompt = \"Benchmark: Provide a short helpful reply.\"\n",
    "N = 5\n",
    "times = []\n",
    "for i in range(N):\n",
    "    payload = {\"prompt\": prompt, \"max_tokens\": 64}\n",
    "    t0 = time.time()\n",
    "    r = requests.post(url, json=payload, timeout=30)\n",
    "    dt = time.time() - t0\n",
    "    times.append(dt)\n",
    "    print(f\"Run {i + 1} status={r.status_code} elapsed={dt:.2f}s\")\n",
    "\n",
    "print(\"Average elapsed\", sum(times) / len(times))\n",
    "print(\n",
    "    \"Tip: Tune --threads, try smaller quantizations (Q2/Q3), or increase --cache-ram to reduce latency.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72460dee",
   "metadata": {},
   "source": [
    "# Quick re-run steps\n",
    "\n",
    "1. Run the **Mount Google Drive** cell above.\n",
    "2. If the signed-in account isn't the desired one, run the `unmount_drive()` helper then re-run the mount cell.\n",
    "3. Run the **Diagnostic: Drive & account verification** cell and paste the output here."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
